{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# example usage: python compute_nhd_routing_SingleSeg.py -v -t -w -n Mainstems_CONUS\n",
    "# python compute_nhd_routing_SingleSeg_v02.py --test -t -v --debuglevel 1\n",
    "# python compute_nhd_routing_SingleSeg_v02.py --test-full-pocono -t -v --debuglevel 1\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NHD Network traversal\n",
    "\n",
    "A demonstration version of this code is stored in this Colaboratory notebook:\n",
    "    https://colab.research.google.com/drive/1ocgg1JiOGBUl3jfSUPCEVnW5WNaqLKCD\n",
    "\n",
    "\"\"\"\n",
    "## Parallel execution\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from joblib import delayed, Parallel\n",
    "from itertools import chain, islice\n",
    "from operator import itemgetter\n",
    "\n",
    "# test\n",
    "supernetwork_parameters=None\n",
    "rconn=None\n",
    "connections=None\n",
    "independent_networks=None \n",
    "reaches_bytw=None\n",
    "param_df= None\n",
    "qlats=None\n",
    "q0=None\n",
    "diffusive_parameters=None\n",
    " #test\n",
    "\n",
    "def _handle_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debuglevel\",\n",
    "        help=\"Set the debuglevel\",\n",
    "        dest=\"debuglevel\",\n",
    "        choices=[0, 1, 2, 3],\n",
    "        default=0,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\",\n",
    "        \"--verbose\",\n",
    "        help=\"Verbose output (leave blank for quiet output)\",\n",
    "        dest=\"verbose\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--qlat-dt\",\n",
    "        \"--qlateral-time-step\",\n",
    "        help=\"Set the default qlateral timestep length\",\n",
    "        dest=\"qdt\",\n",
    "        default=3600,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--qN\",\n",
    "        \"--qts-subdivisions\",\n",
    "        help=\"number of simulation timesteps per qlateral timestep\",\n",
    "        dest=\"qts_subdivisions\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dt\",\n",
    "        \"--simulation-time-step\",\n",
    "        help=\"Set the default simulation timestep length\",\n",
    "        dest=\"dt\",\n",
    "        default=300,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nts\",\n",
    "        \"--number-of-simulation-timesteps\",\n",
    "        help=\"Set the number of timesteps to execute. If used with ql_file or ql_folder, nts must be less than len(ql) x qN.\",\n",
    "        dest=\"nts\",\n",
    "        default=144,\n",
    "        type=int,\n",
    "    )\n",
    "\n",
    "    # change this so after --test, the user enters a test choice\n",
    "    parser.add_argument(\n",
    "        \"--test\",\n",
    "        help=\"Select a test case, routing results will be compared against WRF hydro for parity\",\n",
    "        choices=[\"pocono1\"],\n",
    "        dest=\"test_case\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--sts\",\n",
    "        \"--assume-short-ts\",\n",
    "        help=\"Use the previous timestep value for upstream flow\",\n",
    "        dest=\"assume_short_ts\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ocsv\",\n",
    "        \"--write-output-csv\",\n",
    "        nargs=\"?\",\n",
    "        help=\"Write csv output files to this folder (omit flag for no csv writing)\",\n",
    "        dest=\"csv_output_folder\",\n",
    "        const=\"../../test/output/text\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-t\",\n",
    "        \"--showtiming\",\n",
    "        help=\"Set the showtiming (leave blank for no timing information)\",\n",
    "        dest=\"showtiming\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-w\",\n",
    "        \"--break-at-waterbodies\",\n",
    "        help=\"Use the waterbodies in the route-link dataset to divide the computation (leave blank for no splitting)\",\n",
    "        dest=\"break_network_at_waterbodies\",\n",
    "        action=\"store_true\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--parallel\",\n",
    "        nargs=\"?\",\n",
    "        help=\"Use the parallel computation engine (omit flag for serial computation)\",\n",
    "        dest=\"parallel_compute_method\",\n",
    "        const=\"by-network\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--subnet-size\",\n",
    "        help=\"Set the target size (number of segments) for grouped subnetworks.\",\n",
    "        dest=\"subnetwork_target_size\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cpu-pool\",\n",
    "        help=\"Assign the number of cores to multiprocess across.\",\n",
    "        dest=\"cpu_pool\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--compute-method\",\n",
    "        help=\"Use the cython version of the compute_network code (enter additional flag options for other compute_network possibilities).\",\n",
    "        dest=\"compute_method\",\n",
    "        default=\"standard cython compute network\",\n",
    "    )\n",
    "    supernetwork_arg_group = parser.add_mutually_exclusive_group()\n",
    "    supernetwork_arg_group.add_argument(\n",
    "        \"-n\",\n",
    "        \"--supernetwork\",\n",
    "        help=\"Choose from among the pre-programmed supernetworks (Pocono_TEST1, Pocono_TEST2, LowerColorado_Conchos_FULL_RES, Brazos_LowerColorado_ge5, Brazos_LowerColorado_FULL_RES, Brazos_LowerColorado_Named_Streams, CONUS_ge5, Mainstems_CONUS, CONUS_Named_Streams, CONUS_FULL_RES_v20)\",\n",
    "        choices=[\n",
    "            \"Pocono_TEST1\",\n",
    "            \"Pocono_TEST2\",\n",
    "            \"LowerColorado_Conchos_FULL_RES\",\n",
    "            \"Brazos_LowerColorado_ge5\",\n",
    "            \"Brazos_LowerColorado_FULL_RES\",\n",
    "            \"Brazos_LowerColorado_Named_Streams\",\n",
    "            \"CONUS_ge5\",\n",
    "            \"Mainstems_CONUS\",\n",
    "            \"CONUS_Named_Streams\",\n",
    "            \"CONUS_FULL_RES_v20\",\n",
    "            \"CapeFear_FULL_RES\",\n",
    "            \"Florence_FULL_RES\",\n",
    "        ],\n",
    "        # TODO: accept multiple or a Path (argparse Action perhaps)\n",
    "        # action='append',\n",
    "        # nargs=1,\n",
    "        dest=\"supernetwork\",\n",
    "        default=\"Pocono_TEST1\",\n",
    "    )\n",
    "    supernetwork_arg_group.add_argument(\n",
    "        \"-f\",\n",
    "        \"--custom-input-file\",\n",
    "        dest=\"custom_input_file\",\n",
    "        help=\"OR... please enter the path of a .yaml or .json file containing a custom supernetwork information. See for example test/input/yaml/CustomInput.yaml and test/input/json/CustomInput.json.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-restart-file\",\n",
    "        dest=\"wrf_hydro_channel_restart_file\",\n",
    "        help=\"provide a WRF-Hydro channel warm state file (may be the same as waterbody restart file)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-ID-crosswalk-file\",\n",
    "        dest=\"wrf_hydro_channel_ID_crosswalk_file\",\n",
    "        help=\"provide an xarray-readable file that defines the order of the outputs in the channel restart file. Specify the ID field with --wrf_hydro_channel_ID_crosswalk_file_field_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-ID-crosswalk-file-field-name\",\n",
    "        dest=\"wrf_hydro_channel_ID_crosswalk_file_field_name\",\n",
    "        help=\"Name of the column providing the channel segment IDs in the channel crosswalk file\",\n",
    "        default=\"ID\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-restart-upstream-flow-field-name\",\n",
    "        dest=\"wrf_hydro_channel_restart_upstream_flow_field_name\",\n",
    "        help=\"Name of the column providing the upstream flow at the beginning of the simulation.\",\n",
    "        default=\"qlink1\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-restart-downstream-flow-field-name\",\n",
    "        dest=\"wrf_hydro_channel_restart_downstream_flow_field_name\",\n",
    "        help=\"Name of the column providing the downstream flow at the beginning of the simulation.\",\n",
    "        default=\"qlink2\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wrf-hydro-channel-restart-depth-flow-field-name\",\n",
    "        dest=\"wrf_hydro_channel_restart_depth_flow_field_name\",\n",
    "        help=\"Name of the column providing the depth of flow at the beginning of the simulation.\",\n",
    "        default=\"hlink\",\n",
    "    )\n",
    "    # TODO: Refine exclusivity of ql args (currently not going to accept more than one arg; more than one is needed for qlw, for instance.)\n",
    "    ql_arg_group = parser.add_mutually_exclusive_group()\n",
    "    ql_arg_group.add_argument(\n",
    "        \"--qlc\",\n",
    "        \"--constant_qlateral\",\n",
    "        help=\"Constant qlateral to apply to all time steps at all segments\",\n",
    "        dest=\"qlat_const\",\n",
    "        type=float,\n",
    "        default=10,\n",
    "    )\n",
    "    ql_arg_group.add_argument(\n",
    "        \"--qlf\",\n",
    "        \"--single_file_qlateral\",\n",
    "        help=\"QLaterals arranged with segment IDs as rows and timesteps as columns in a single .csv\",\n",
    "        dest=\"qlat_input_file\",\n",
    "    )\n",
    "    ql_arg_group.add_argument(\n",
    "        \"--qlw\",\n",
    "        \"--ql_wrf_hydro_folder\",\n",
    "        help=\"QLaterals in separate netcdf files as found in standard WRF-Hydro output\",\n",
    "        dest=\"qlat_input_folder\",\n",
    "    )\n",
    "    ql_arg_group.add_argument(\n",
    "        \"--qlic\",\n",
    "        \"--qlat_file_index_col\",\n",
    "        help=\"QLateral index column number\",\n",
    "        dest=\"qlat_file_index_col\",\n",
    "        default=\"feature_id\",\n",
    "    )\n",
    "    ql_arg_group.add_argument(\n",
    "        \"--qlvc\",\n",
    "        \"--qlat_file_value_col\",\n",
    "        help=\"QLateral value column number\",\n",
    "        dest=\"qlat_file_value_col\",\n",
    "        default=\"q_lateral\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--qlat_file_pattern_filter\",\n",
    "        help=\"Provide a globbing pattern to identify files in the Wrf-Hydro qlateral output file folder\",\n",
    "        dest=\"qlat_file_pattern_filter\",\n",
    "        default=\"q_lateral\",\n",
    "    )\n",
    "    parser.add_argument(\"--ql\", help=\"QLat input data\", dest=\"ql\", default=None)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "ENV_IS_CL = False\n",
    "if ENV_IS_CL:\n",
    "    root = pathlib.Path(\"/\", \"content\", \"t-route\")\n",
    "elif not ENV_IS_CL:\n",
    "    root = pathlib.Path(\"../..\").resolve()\n",
    "    # sys.path.append(r\"../python_framework_v02\")\n",
    "\n",
    "    # TODO: automate compile for the package scripts\n",
    "    sys.path.append(\"fast_reach\")\n",
    "\n",
    "## network and reach utilities\n",
    "import troute.nhd_network_utilities_v02 as nnu\n",
    "import mc_reach\n",
    "import troute.nhd_network as nhd_network\n",
    "import troute.nhd_io as nhd_io\n",
    "import build_tests  # TODO: Determine whether and how to incorporate this into setup.py\n",
    "import troute.nhd_network_augment as nna\n",
    "\n",
    "def writetoFile(file, writeString):\n",
    "    file.write(writeString)\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def constant_qlats(index_dataset, nsteps, qlat):\n",
    "    q = np.full((len(index_dataset.index), nsteps), qlat, dtype=\"float32\")\n",
    "    ql = pd.DataFrame(q, index=index_dataset.index, columns=range(nsteps))\n",
    "    return ql\n",
    "\n",
    "\n",
    "def diffusive_routing_v02(\n",
    "    connections,\n",
    "    rconn,\n",
    "    reaches_bytw,\n",
    "    compute_func,\n",
    "    parallel_compute_method,\n",
    "    subnetwork_target_size,\n",
    "    cpu_pool,\n",
    "    nts,\n",
    "    qts_subdivisions,\n",
    "    independent_networks,\n",
    "    param_df,\n",
    "    qlats,\n",
    "    q0,\n",
    "    assume_short_ts,\n",
    "):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if parallel_compute_method == \"by-network\":\n",
    "        with Parallel(n_jobs=cpu_pool, backend=\"threading\") as parallel:\n",
    "            jobs = []\n",
    "            for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "                segs = list(chain.from_iterable(reach_list))\n",
    "                param_df_sub = param_df.loc[\n",
    "                    segs, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "                ].sort_index()\n",
    "                qlat_sub = qlats.loc[segs].sort_index()\n",
    "                q0_sub = q0.loc[segs].sort_index()\n",
    "                jobs.append(\n",
    "                    delayed(compute_func)(\n",
    "                        nts,\n",
    "                        qts_subdivisions,\n",
    "                        reach_list,\n",
    "                        independent_networks[tw],\n",
    "                        param_df_sub.index.values,\n",
    "                        param_df_sub.columns.values,\n",
    "                        param_df_sub.values,\n",
    "                        qlat_sub.values,\n",
    "                        q0_sub.values,\n",
    "                        {},\n",
    "                        assume_short_ts,\n",
    "                    )\n",
    "                )\n",
    "            results = parallel(jobs)\n",
    "\n",
    "    else:  # Execute in serial\n",
    "        results = []\n",
    "        for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "            segs = list(chain.from_iterable(reach_list))\n",
    "            param_df_sub = param_df.loc[\n",
    "                segs, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "            ].sort_index()\n",
    "            qlat_sub = qlats.loc[segs].sort_index()\n",
    "            q0_sub = q0.loc[segs].sort_index()\n",
    "            \n",
    "            if tw==8777215:\n",
    "                print(f\"tw:{tw} reach_list{reach_list}\")\n",
    "                results.append(\n",
    "                    compute_func(\n",
    "                        nts,\n",
    "                        qts_subdivisions,\n",
    "                        reach_list,\n",
    "                        independent_networks[tw],\n",
    "                        param_df_sub.index.values,\n",
    "                        param_df_sub.columns.values,\n",
    "                        param_df_sub.values,\n",
    "                        qlat_sub.values,\n",
    "                        q0_sub.values,\n",
    "                        {},\n",
    "                        assume_short_ts,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _input_handler():\n",
    "\n",
    "    #args = _handle_args()\n",
    "\n",
    "    #custom_input_file = args.custom_input_file\n",
    "    custom_input_file=\"../../test/input/yaml/CustomInput_florence_933020089_dt300.yaml\"\n",
    "    \n",
    "    supernetwork_parameters = {}\n",
    "    waterbody_parameters = {}\n",
    "    forcing_parameters = {}\n",
    "    restart_parameters = {}\n",
    "    output_parameters = {}\n",
    "    run_parameters = {}\n",
    "    parity_parameters = {}\n",
    "    diffusive_parameters={}\n",
    "\n",
    "    if custom_input_file:\n",
    "        (\n",
    "            supernetwork_parameters,\n",
    "            waterbody_parameters,\n",
    "            forcing_parameters,\n",
    "            restart_parameters,\n",
    "            output_parameters,\n",
    "            run_parameters,\n",
    "            parity_parameters,\n",
    "            diffusive_parameters,\n",
    "        ) = nhd_io.read_custom_input(custom_input_file)\n",
    "        run_parameters[\"debuglevel\"] *= -1\n",
    "        print(f\"in here: custom_input_file\")\n",
    "\n",
    "    else:\n",
    "        print(f\"deleted here\")\n",
    "    \n",
    "    return (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,\n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "        parity_parameters,\n",
    "        diffusive_parameters,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    global supernetwork_parameters #test\n",
    "    global rconn #test\n",
    "    global connections #test\n",
    "    global independent_networks #test \n",
    "    global reaches_bytw #test\n",
    "    global param_df\n",
    "    global qlats\n",
    "    global q0\n",
    "    global diffusive_parameters\n",
    "    \n",
    "    (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,\n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "        parity_parameters,\n",
    "        diffusive_parameters,\n",
    "    ) = _input_handler()\n",
    "\n",
    "   \n",
    "    dt = run_parameters.get(\"dt\", None)\n",
    "    nts = run_parameters.get(\"nts\", None)\n",
    "    verbose = run_parameters.get(\"verbose\", None)\n",
    "    showtiming = run_parameters.get(\"showtiming\", None)\n",
    "    debuglevel = run_parameters.get(\"debuglevel\", 0)\n",
    "    \n",
    "\n",
    "    geo_file_path = supernetwork_parameters.get(\"geo_file_path\", None)\n",
    "    print(f\"geo_file_path:{geo_file_path}\")    \n",
    "\n",
    "    if verbose:\n",
    "        print(\"creating supernetwork connections set\")\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 1: Build basic network connections graph\n",
    "    connections, wbodies, param_df = nnu.build_connections(supernetwork_parameters, dt)\n",
    "    #print(connections)\n",
    "     \n",
    "    if verbose:\n",
    "        print(\"supernetwork connections set complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    # STEP 2: Identify Independent Networks and Reaches by Network\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"organizing connections into reaches ...\")\n",
    "\n",
    "    independent_networks, reaches_bytw, rconn = nnu.organize_independent_networks(\n",
    "        connections\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"reach organization complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    # STEP 4: Handle Channel Initial States\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"setting channel initial states ...\")\n",
    "\n",
    "    q0 = nnu.build_channel_initial_state(restart_parameters, param_df.index)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"channel initial states complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 5: Read (or set) QLateral Inputs\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"creating qlateral array ...\")\n",
    "\n",
    "    qlats = nnu.build_qlateral_array(forcing_parameters, connections.keys(), nts)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"qlateral array complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    ################### Main Execution Loop across ordered networks\n",
    "    if showtiming:\n",
    "        main_start_time = time.time()\n",
    "    if verbose:\n",
    "        print(f\"executing routing computation ...\")\n",
    "\n",
    "    #if run_parameters.get(\"compute_method\", None) == \"standard cython compute network\":\n",
    "    #    compute_func = mc_reach.compute_network\n",
    "    #else:\n",
    "    #    compute_func = mc_reach.compute_network\n",
    "\n",
    "    #results = diffusive_routing_v02(\n",
    "    #    connections,\n",
    "    #    rconn,\n",
    "    #    reaches_bytw,\n",
    "    #    compute_func,\n",
    "    #    run_parameters.get(\"parallel_compute_method\", None),\n",
    "    #    run_parameters.get(\"subnetwork_target_size\", 1),\n",
    "        # The default here might be the whole network or some percentage...\n",
    "    #    run_parameters.get(\"cpu_pool\", None),\n",
    "    #    run_parameters.get(\"nts\", 1),\n",
    "    #    run_parameters.get(\"qts_subdivisions\", 1),\n",
    "    #    independent_networks,\n",
    "    #    param_df,\n",
    "    #    qlats,\n",
    "    #    q0,\n",
    "    #    run_parameters.get(\"assume_short_ts\", False),\n",
    "    #)\n",
    "\n",
    "    #csv_output_folder = output_parameters.get(\"csv_output_folder\", None)\n",
    "    #if (debuglevel <= -1) or csv_output_folder:\n",
    "    #    qvd_columns = pd.MultiIndex.from_product(\n",
    "    #        [range(nts), [\"q\", \"v\", \"d\"]]\n",
    "    #    ).to_flat_index()\n",
    "    #    flowveldepth = pd.concat(\n",
    "    #        [pd.DataFrame(d, index=i, columns=qvd_columns) for i, d in results],\n",
    "    #        copy=False,\n",
    "    #    )\n",
    "\n",
    "    #    if csv_output_folder:\n",
    "    #        flowveldepth = flowveldepth.sort_index()\n",
    "    #        output_path = pathlib.Path(csv_output_folder).resolve()\n",
    "    #        flowveldepth.to_csv(output_path.joinpath(f\"{args.supernetwork}.csv\"))\n",
    "\n",
    "    #    if debuglevel <= -1:\n",
    "    #        print(flowveldepth)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ordered reach computation complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    if not \"parity_check_input_folder\" in parity_parameters:\n",
    "        if verbose:\n",
    "            print(\n",
    "\n",
    "                \"conducting parity check, comparing WRF Hydro results against t-route results\"\n",
    "            )\n",
    "\n",
    "        build_tests.parity_check(\n",
    "            parity_parameters, run_parameters[\"nts\"], run_parameters[\"dt\"], results,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import RouteLink_adjustment_v02 as rladj\n",
    "import fortran_python_map_v02 as fpm\n",
    "from pyuniflowtzlt import uniflow_lookuptable\n",
    "\n",
    "usgs_retrievaltool_path= diffusive_parameters.get(\"usgs_retrievaltool_path\",None)\n",
    "sys.path.append(usgs_retrievaltool_path)\n",
    "#results = []\n",
    "# retrieve time step info.\n",
    "dt_ql_g=diffusive_parameters.get(\"dt_qlat\",None) # time step of lateral flow\n",
    "dt_ub_g=diffusive_parameters.get(\"dt_upstream_boundary\",None) # time step of us.boundary data\n",
    "dt_db_g=diffusive_parameters.get(\"dt_downstream_boundary\",None) # time step of ds.boundary data\n",
    "saveinterval_g=diffusive_parameters.get(\"dt_output\",None) # time step of outputting routed results\n",
    "dtini_g=diffusive_parameters.get(\"dt_diffusive\",None) # initial simulation time step \n",
    "# USGS data related info.\n",
    "usgsID= diffusive_parameters.get(\"usgsID\",None)\n",
    "seg2usgsID= diffusive_parameters.get(\"link2usgsID\",None)\n",
    "usgssDT= diffusive_parameters.get(\"diffusive_start_date\",None)\n",
    "usgseDT= diffusive_parameters.get(\"diffusive_end_date\",None)\n",
    "usgspCd= diffusive_parameters.get(\"usgs_parameterCd\",None)\n",
    "\n",
    "for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "    \n",
    "    #if tw==8777215 or tw==166737669:\n",
    "    #if tw==8777215 or tw==933020089:\n",
    "    if tw==933020089:\n",
    "    #if tw==8777215:\n",
    "    #if tw==166737669:\n",
    "        # downstream boundary (tw) segment ID -> make an additional fake tw segment\n",
    "        dbfksegID= str(tw)+ str(2)\n",
    "        dbfksegID= int(dbfksegID) \n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    #                                          CREATE:\n",
    "    #\n",
    "    # ordered_reaches-> {junc.order 1:[ [head_seg1, {key:value...}], ..., [head_seg m, {key:value...}] ],\n",
    "    #                       ....     :     .....                  \n",
    "    #                    junc.order n:[ [head_seg1, {key:value...}], ..., [head_seg l, {key:value...}] ]}\n",
    "    #                      = i[0]    :[ [ i[1][0] ,               ]                                    ]}    \n",
    "    # 'segments_list'= list of segments of a reach from up to downstream \n",
    "    # 'upstream_bottom_segments' = bottom segments of upstream reaches either at a junction or a headbasin.\n",
    "    # 'downstream_head_segments' = head segment of a downstream reach either at a junction or TW. \n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "        ordered_reaches={}\n",
    "        rchhead_reaches={}\n",
    "        rchbottom_reaches={}\n",
    "        z_all={}\n",
    "    \n",
    "        flat_list=list(itertools.chain(*reaches_bytw[tw])) # a list of all segments of reaches_bytw for a given tw.\n",
    "        rconn_tw={key:value for key, value in rconn.items() if key in flat_list} # subset rconn by flat_list\n",
    "        connections_tw= {key:value for key, value in connections.items() if key in flat_list} # subset connections by flat_list\n",
    "\n",
    "        path_func = partial(nhd_network.split_at_junction, rconn_tw)\n",
    "        tr = nhd_network.dfs_decomposition_depth_tuple(rconn_tw, path_func)\n",
    "        jorder_reaches_tw=sorted(tr, key=lambda x: x[0]) # [ (jorder:[segments]), ... , (jorder:[segments]) ] \n",
    "        \n",
    "        mx_jorder_tw=max(jorder_reaches_tw)[0] # maximum junction order of subnetwork of TW\n",
    "        nrch_g=len(jorder_reaches_tw) # the number of reaches        \n",
    "        maxlist=max(jorder_reaches_tw, key=lambda i:len(i[1]))\n",
    "        mxncomp_g= len(maxlist[1])+1 # max. number of nodes (segments+one additional segment) within a reach\n",
    "                    \n",
    "        for i in jorder_reaches_tw:\n",
    "            # add one more segment(fake) to the end of a list of segments to account for node configuration.\n",
    "            fksegID= i[1][len(i[1])-1]\n",
    "            fksegID= int(str(fksegID) + str(2))\n",
    "            i[1].append(fksegID)\n",
    "            # additional segment(fake) to upstream bottom segments\n",
    "            fk_usbseg=[int(str(x)+str(2)) for x in rconn_tw[i[1][0]]]            \n",
    "            \n",
    "            if i[0] not in ordered_reaches:\n",
    "                ordered_reaches.update({i[0]:[]})\n",
    "            # 1) For 'downstream'\n",
    "            ordered_reaches[i[0]].append([i[1][0],{'number_segments':len(i[1]),\\\n",
    "                                        'segments_list':i[1],\\\n",
    "                                        'upstream_bottom_segments':fk_usbseg,\\\n",
    "                                        'downstream_head_segment':connections_tw[i[1][len(i[1])-2]]}]) \n",
    "                     \n",
    "            if i[1][0] not in rchhead_reaches:    \n",
    "            # a list of segments for a given head segment\n",
    "                rchhead_reaches.update({i[1][0]:{\"number_segments\":len(i[1]),\\\n",
    "                                            \"segments_list\":i[1]}})\n",
    "            # a list of segments for a given bottom segment\n",
    "                rchbottom_reaches.update({i[1][len(i[1])-1]:{\"number_segments\":len(i[1]),\\\n",
    "                                                     \"segments_list\":i[1]}})\n",
    "           \n",
    "            # for channel altitude adjustment\n",
    "            z_all.update({seg:{'adj.alt':np.zeros(1)}\n",
    "                                        for seg in i[1]})\n",
    "        # cahnnel geometry data\n",
    "        ch_geo_data_tw = param_df.loc[\n",
    "        flat_list, [\"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\", \"alt\"]]\n",
    "        ch_geo_data_tw[:][\"cs\"]= 1.0/ch_geo_data_tw[:][\"cs\"]\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-3           \n",
    "\n",
    "    #    Adjust altitude so that altitude of the last sement of a reach is equal to that \n",
    "    #    of the first segment of its downstream reach right after their common junction.\n",
    "    #--------------------------------------------------------------------------------------\n",
    "        rladj.adj_alt1(mx_jorder_tw\n",
    "                    , ordered_reaches\n",
    "                    , ch_geo_data_tw\n",
    "                    , dbfksegID\n",
    "                    , z_all\n",
    "                    ) \n",
    "    #--------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-4           \n",
    "\n",
    "    #     Make Fortran-Python channel network mapping variables.\n",
    "    #--------------------------------------------------------------------------------------   \n",
    "        pynw={}\n",
    "        frj=-1\n",
    "        for x in range(mx_jorder_tw,-1,-1): \n",
    "            for head_segment, reach in ordered_reaches[x]:\n",
    "                frj= frj+1\n",
    "                pynw[frj]=head_segment\n",
    "        \n",
    "        frnw_col=8\n",
    "        frnw_g=fpm.fp_network_map(mx_jorder_tw\n",
    "                , ordered_reaches\n",
    "                , rchbottom_reaches\n",
    "                , nrch_g\n",
    "                , frnw_col\n",
    "                , dbfksegID\n",
    "                , pynw\n",
    "                )  \n",
    "        #covert data type from integer to float for frnw\n",
    "        dfrnw_g=np.zeros((nrch_g,frnw_col), dtype=float)\n",
    "        for j in range(0,nrch_g):\n",
    "            for col in range(0,frnw_col):\n",
    "                dfrnw_g[j,col]=float(frnw_g[j,col])\n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-5\n",
    "\n",
    "    #                  Prepare channel geometry data           \n",
    "    #---------------------------------------------------------------------------------    \n",
    "        z_ar_g, bo_ar_g, traps_ar_g, tw_ar_g, twcc_ar_g, mann_ar_g, manncc_ar_g, so_ar_g, dx_ar_g= fpm.fp_chgeo_map(mx_jorder_tw\n",
    "                    , ordered_reaches\n",
    "                    , ch_geo_data_tw\n",
    "                    , z_all\n",
    "                    , mxncomp_g\n",
    "                    , nrch_g                    \n",
    "                    )   \n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-6\n",
    "\n",
    "    #                  Prepare lateral inflow data           \n",
    "    #---------------------------------------------------------------------------------\n",
    "        segs = list(chain.from_iterable(reach_list))\n",
    "        qlat_tw = qlats.loc[segs]    \n",
    "        tfin_g=len(qlat_tw.columns)-1 #entire simulation period in hrs\n",
    "        nts_ql_g= tfin_g*3600.0//int(dt_ql_g)+1.0 # the number of the entire time steps of lateral flow data \n",
    "        nts_ql_g= int(nts_ql_g)\n",
    "        qlat_g=np.zeros((nts_ql_g, mxncomp_g, nrch_g)) \n",
    "        \n",
    "        fpm.fp_qlat_map(mx_jorder_tw\n",
    "            , ordered_reaches\n",
    "            , nts_ql_g\n",
    "            , qlat_tw            \n",
    "            , qlat_g\n",
    "            ) \n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-7\n",
    "\n",
    "    #       Prepare upstream boundary (top segments of head basin reaches) data            \n",
    "    #---------------------------------------------------------------------------------\n",
    "        nts_ub_g= nts_ql_g \n",
    "        ubcd_g = fpm.fp_ubcd_map(frnw_g\n",
    "                                , pynw\n",
    "                                , nts_ub_g\n",
    "                                , nrch_g\n",
    "                                , ch_geo_data_tw\n",
    "                                , qlat_tw\n",
    "                                , qlat_g\n",
    "                                )\n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-8\n",
    "\n",
    "    #       Prepare downstrea boundary (bottom segments of TW reaches) data            \n",
    "    #---------------------------------------------------------------------------------        \n",
    "        if tw in seg2usgsID:\n",
    "            ipos= seg2usgsID.index(tw)\n",
    "            usgsID2tw= usgsID[ipos]         \n",
    "            nts_db_g, dbcd_g=fpm.fp_dbcd_map(usgsID2tw\n",
    "                        , usgssDT\n",
    "                        , usgseDT\n",
    "                        , usgspCd\n",
    "                        )\n",
    "        else:\n",
    "            nts_db_g=-1.0     \n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-8\n",
    "\n",
    "    #                 Prepare uniform flow lookup tables            \n",
    "    #---------------------------------------------------------------------------------          \n",
    "        nhincr_m_g=20\n",
    "        nhincr_f_g=20               \n",
    "        timesdepth_g=10\n",
    "        ufhlt_m_g, ufqlt_m_g, ufhlt_f_g, ufqlt_f_g= uniflow_lookuptable(mxncomp_g \n",
    "                                                                    , nrch_g \n",
    "                                                                    , bo_ar_g \n",
    "                                                                    , traps_ar_g \n",
    "                                                                    , tw_ar_g \n",
    "                                                                    , twcc_ar_g \n",
    "                                                                    , mann_ar_g \n",
    "                                                                    , manncc_ar_g \n",
    "                                                                    , so_ar_g \n",
    "                                                                    , nhincr_m_g \n",
    "                                                                    , nhincr_f_g               \n",
    "                                                                    , frnw_col \n",
    "                                                                    , dfrnw_g \n",
    "                                                                    , timesdepth_g)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ufhlt_m_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
