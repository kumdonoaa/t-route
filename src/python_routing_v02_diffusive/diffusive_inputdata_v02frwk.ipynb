{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# example usage: python compute_nhd_routing_SingleSeg.py -v -t -w -n Mainstems_CONUS\n",
    "# python compute_nhd_routing_SingleSeg_v02.py --test -t -v --debuglevel 1\n",
    "# python compute_nhd_routing_SingleSeg_v02.py --test-full-pocono -t -v --debuglevel 1\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NHD Network traversal\n",
    "\n",
    "A demonstration version of this code is stored in this Colaboratory notebook:\n",
    "    https://colab.research.google.com/drive/1ocgg1JiOGBUl3jfSUPCEVnW5WNaqLKCD\n",
    "\n",
    "\"\"\"\n",
    "## Parallel execution\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from joblib import delayed, Parallel\n",
    "from itertools import chain, islice\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "ENV_IS_CL = False\n",
    "if ENV_IS_CL:\n",
    "    root = pathlib.Path(\"/\", \"content\", \"t-route\")\n",
    "elif not ENV_IS_CL:\n",
    "    #root = pathlib.Path(\"../..\").resolve()\n",
    "    # sys.path.append(r\"../python_framework_v02\")\n",
    "    # TODO: automate compile for the package scripts\n",
    "    #sys.path.append(\"fast_reach\")\n",
    "    root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "    sys.path.append(os.path.join(root, r'src', r'python_framework_v02','troute'))\n",
    "    sys.path.append(os.path.join(root, r'src', r'python_routing_v02','fast_reach'))\n",
    "\n",
    "\n",
    "import troute.nhd_network_utilities_v02 as nnu\n",
    "import mc_reach\n",
    "import troute.nhd_network as nhd_network\n",
    "import troute.nhd_io as nhd_io\n",
    "import build_tests  # TODO: Determine whether and how to incorporate this into setup.py\n",
    "# import troute.nhd_network_augment as nna\n",
    "\n",
    "\"\"\"\n",
    "def writetoFile(file, writeString):\n",
    "    file.write(writeString)\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def constant_qlats(index_dataset, nsteps, qlat):\n",
    "    q = np.full((len(index_dataset.index), nsteps), qlat, dtype=\"float32\")\n",
    "    ql = pd.DataFrame(q, index=index_dataset.index, columns=range(nsteps))\n",
    "    return ql\n",
    "\n",
    "def diffusive_routing_v02(\n",
    "    connections,\n",
    "    rconn,\n",
    "    reaches_bytw,\n",
    "    compute_func,\n",
    "    parallel_compute_method,\n",
    "    subnetwork_target_size,\n",
    "    cpu_pool,\n",
    "    nts,\n",
    "    qts_subdivisions,\n",
    "    independent_networks,\n",
    "    param_df,\n",
    "    qlats,\n",
    "    q0,\n",
    "    assume_short_ts,\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if parallel_compute_method == \"by-network\":\n",
    "        with Parallel(n_jobs=cpu_pool, backend=\"threading\") as parallel:\n",
    "            jobs = []\n",
    "            for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "                segs = list(chain.from_iterable(reach_list))\n",
    "                param_df_sub = param_df.loc[\n",
    "                    segs, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "                ].sort_index()\n",
    "                qlat_sub = qlats.loc[segs].sort_index()\n",
    "                q0_sub = q0.loc[segs].sort_index()\n",
    "                jobs.append(\n",
    "                    delayed(compute_func)(\n",
    "                        nts,\n",
    "                        qts_subdivisions,\n",
    "                        reach_list,\n",
    "                        independent_networks[tw],\n",
    "                        param_df_sub.index.values,\n",
    "                        param_df_sub.columns.values,\n",
    "                        param_df_sub.values,\n",
    "                        qlat_sub.values,\n",
    "                        q0_sub.values,\n",
    "                        {},\n",
    "                        assume_short_ts,\n",
    "                    )\n",
    "                )\n",
    "            results = parallel(jobs)\n",
    "\n",
    "    else:  # Execute in serial\n",
    "        results = []\n",
    "        for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "            segs = list(chain.from_iterable(reach_list))\n",
    "            param_df_sub = param_df.loc[\n",
    "                segs, [\"dt\", \"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\"]\n",
    "            ].sort_index()\n",
    "            qlat_sub = qlats.loc[segs].sort_index()\n",
    "            q0_sub = q0.loc[segs].sort_index()\n",
    "            \n",
    "            if tw==8777215:\n",
    "                print(f\"tw:{tw} reach_list{reach_list}\")\n",
    "                results.append(\n",
    "                    compute_func(\n",
    "                        nts,\n",
    "                        qts_subdivisions,\n",
    "                        reach_list,\n",
    "                        independent_networks[tw],\n",
    "                        param_df_sub.index.values,\n",
    "                        param_df_sub.columns.values,\n",
    "                        param_df_sub.values,\n",
    "                        qlat_sub.values,\n",
    "                        q0_sub.values,\n",
    "                        {},\n",
    "                        assume_short_ts,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    return results\n",
    "\"\"\"\n",
    "def diffusive_input_data_v02(connections\n",
    "                            , rconn\n",
    "                            , reaches_bytw\n",
    "                            , diffusive_parameters\n",
    "                            , param_df\n",
    "                            , qlats\n",
    "                            ):\n",
    "\n",
    "    start_time= time.time()\n",
    "    \n",
    "    import itertools\n",
    "    import RouteLink_adjustment_v02 as rladj\n",
    "    import fortran_python_map_v02 as fpm\n",
    "    #from pyuniflowtzlt import uniflow_lookuptable\n",
    "\n",
    "    usgs_retrievaltool_path= diffusive_parameters.get(\"usgs_retrievaltool_path\",None)\n",
    "    sys.path.append(usgs_retrievaltool_path)\n",
    "    #results = []\n",
    "    # diffusive time steps info.\n",
    "    dt_ql_g=diffusive_parameters.get(\"dt_qlat\",None) # time step of lateral flow\n",
    "    dt_ub_g=diffusive_parameters.get(\"dt_upstream_boundary\",None) # time step of us.boundary data\n",
    "    dt_db_g=diffusive_parameters.get(\"dt_downstream_boundary\",None) # time step of ds.boundary data\n",
    "    saveinterval_g=diffusive_parameters.get(\"dt_output\",None) # time step for outputting routed results\n",
    "    saveinterval_ev_g=diffusive_parameters.get(\"dt_output\",None) # time step for evaluating routed results\n",
    "    dtini_g=diffusive_parameters.get(\"dt_diffusive\",None) # initial simulation time step\n",
    "    t0_g=0.0 #simulation start hr **set to zero for Fortran computation \n",
    "    tfin_g=diffusive_parameters.get(\"simulation_end_hr\",None) # simulation end time\n",
    "\n",
    "    # USGS data related info.\n",
    "    usgsID= diffusive_parameters.get(\"usgsID\",None)\n",
    "    seg2usgsID= diffusive_parameters.get(\"link2usgsID\",None)\n",
    "    usgssDT= diffusive_parameters.get(\"usgs_start_date\",None)\n",
    "    usgseDT= diffusive_parameters.get(\"usgs_end_date\",None)\n",
    "    usgspCd= diffusive_parameters.get(\"usgs_parameterCd\",None)\n",
    "\n",
    "    # diffusive parameters\n",
    "    cfl_g= diffusive_parameters.get(\"courant_number_upper_limit\",None)\n",
    "    theta_g= diffusive_parameters.get(\"theta_parameter\",None)\n",
    "    tzeq_flag_g= diffusive_parameters.get(\"chgeo_computation_flag\",None)\n",
    "    y_opt_g= diffusive_parameters.get(\"water_elevation_computation_flag\",None)\n",
    "    so_llm_g= diffusive_parameters.get(\"bed_slope_lower_limit\",None)\n",
    "\n",
    "    for twi, (tw, reach_list) in enumerate(reaches_bytw.items(), 1):\n",
    "    \n",
    "        if tw==933020089:  #if tw==8777215 or tw==166737669:\n",
    "            # downstream boundary (tw) segment ID -> make an additional fake tw segment\n",
    "            dbfksegID= str(tw)+ str(2)\n",
    "            dbfksegID= int(dbfksegID) \n",
    "            ordered_reaches={}\n",
    "            rchhead_reaches={}\n",
    "            rchbottom_reaches={}\n",
    "            z_all={}\n",
    "    \n",
    "            flat_list=list(itertools.chain(*reaches_bytw[tw])) # a list of all segments of reaches_bytw for a given tw.\n",
    "            rconn_tw={key:value for key, value in rconn.items() if key in flat_list} # subset rconn by flat_list\n",
    "            connections_tw= {key:value for key, value in connections.items() if key in flat_list} # subset connections by flat_list\n",
    "\n",
    "            path_func = partial(nhd_network.split_at_junction, rconn_tw)\n",
    "            tr = nhd_network.dfs_decomposition_depth_tuple(rconn_tw, path_func)\n",
    "            jorder_reaches_tw=sorted(tr, key=lambda x: x[0]) # [ (jorder:[segments]), ... , (jorder:[segments]) ] \n",
    "\n",
    "            mx_jorder_tw=max(jorder_reaches_tw)[0] # maximum junction order of subnetwork of TW\n",
    "            nrch_g=len(jorder_reaches_tw) # the number of reaches        \n",
    "            maxlist=max(jorder_reaches_tw, key=lambda i:len(i[1]))\n",
    "            mxncomp_g= len(maxlist[1])+1 # max. number of nodes (segments+one additional segment) within a reach\n",
    "\n",
    "            for i in jorder_reaches_tw:\n",
    "                # add one more segment(fake) to the end of a list of segments to account for node configuration.\n",
    "                fksegID= i[1][len(i[1])-1]\n",
    "                fksegID= int(str(fksegID) + str(2))\n",
    "                i[1].append(fksegID)\n",
    "                # additional segment(fake) to upstream bottom segments\n",
    "                fk_usbseg=[int(str(x)+str(2)) for x in rconn_tw[i[1][0]]]            \n",
    "\n",
    "                if i[0] not in ordered_reaches:\n",
    "                    ordered_reaches.update({i[0]:[]})\n",
    "                ordered_reaches[i[0]].append([i[1][0],{'number_segments':len(i[1]),\\\n",
    "                                            'segments_list':i[1],\\\n",
    "                                            'upstream_bottom_segments':fk_usbseg,\\\n",
    "                                            'downstream_head_segment':connections_tw[i[1][len(i[1])-2]]}]) \n",
    "\n",
    "                if i[1][0] not in rchhead_reaches:    \n",
    "                    # a list of segments for a given head segment\n",
    "                    rchhead_reaches.update({i[1][0]:{\"number_segments\":len(i[1]),\\\n",
    "                                                \"segments_list\":i[1]}})\n",
    "                    # a list of segments for a given bottom segment\n",
    "                    rchbottom_reaches.update({i[1][len(i[1])-1]:{\"number_segments\":len(i[1]),\\\n",
    "                                                         \"segments_list\":i[1]}})\n",
    "                # for channel altitude adjustment\n",
    "                z_all.update({seg:{'adj.alt':np.zeros(1)}\n",
    "                                            for seg in i[1]})\n",
    "            # cahnnel geometry data\n",
    "            ch_geo_data_tw = param_df.loc[\n",
    "            flat_list, [\"bw\", \"tw\", \"twcc\", \"dx\", \"n\", \"ncc\", \"cs\", \"s0\", \"alt\"]]\n",
    "            ch_geo_data_tw[:][\"cs\"]= 1.0/ch_geo_data_tw[:][\"cs\"]\n",
    "        #--------------------------------------------------------------------------------------\n",
    "        #                                 Step 0-3           \n",
    "\n",
    "        #    Adjust altitude so that altitude of the last sement of a reach is equal to that \n",
    "        #    of the first segment of its downstream reach right after their common junction.\n",
    "        #--------------------------------------------------------------------------------------\n",
    "            rladj.adj_alt1(mx_jorder_tw\n",
    "                        , ordered_reaches\n",
    "                        , ch_geo_data_tw\n",
    "                        , dbfksegID\n",
    "                        , z_all\n",
    "                        ) \n",
    "        #--------------------------------------------------------------------------------------\n",
    "        #                                 Step 0-4           \n",
    "\n",
    "        #     Make Fortran-Python channel network mapping variables.\n",
    "        #--------------------------------------------------------------------------------------   \n",
    "            pynw={}\n",
    "            frj=-1\n",
    "            for x in range(mx_jorder_tw,-1,-1): \n",
    "                for head_segment, reach in ordered_reaches[x]:\n",
    "                    frj= frj+1\n",
    "                    pynw[frj]=head_segment\n",
    "\n",
    "            #frnw_col=8\n",
    "            frnw_col= diffusive_parameters.get(\"fortran_nework_map_col_number\",None)\n",
    "            frnw_g=fpm.fp_network_map(mx_jorder_tw\n",
    "                    , ordered_reaches\n",
    "                    , rchbottom_reaches\n",
    "                    , nrch_g\n",
    "                    , frnw_col\n",
    "                    , dbfksegID\n",
    "                    , pynw\n",
    "                    )  \n",
    "            #covert data type from integer to float for frnw\n",
    "            dfrnw_g=np.zeros((nrch_g,frnw_col), dtype=float)\n",
    "            for j in range(0,nrch_g):\n",
    "                for col in range(0,frnw_col):\n",
    "                    dfrnw_g[j,col]=float(frnw_g[j,col])\n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-5\n",
    "\n",
    "        #                  Prepare channel geometry data           \n",
    "        #---------------------------------------------------------------------------------    \n",
    "            z_ar_g, bo_ar_g, traps_ar_g, tw_ar_g, twcc_ar_g, mann_ar_g, manncc_ar_g, so_ar_g, dx_ar_g= fpm.fp_chgeo_map(mx_jorder_tw\n",
    "                        , ordered_reaches\n",
    "                        , ch_geo_data_tw\n",
    "                        , z_all\n",
    "                        , mxncomp_g\n",
    "                        , nrch_g                    \n",
    "                        )   \n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-6\n",
    "\n",
    "        #                  Prepare lateral inflow data           \n",
    "        #---------------------------------------------------------------------------------\n",
    "            segs = list(chain.from_iterable(reach_list))\n",
    "            qlat_tw = qlats.loc[segs]    \n",
    "            #tfin_g=len(qlat_tw.columns)-1 #entire simulation period in hrs\n",
    "            nts_ql_g= int((tfin_g-t0_g)*3600.0/dt_ql_g)+1 # the number of the entire time steps of lateral flow data \n",
    "\n",
    "            qlat_g=np.zeros((nts_ql_g, mxncomp_g, nrch_g)) \n",
    "\n",
    "            fpm.fp_qlat_map(mx_jorder_tw\n",
    "                , ordered_reaches\n",
    "                , nts_ql_g\n",
    "                , qlat_tw            \n",
    "                , qlat_g\n",
    "                ) \n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-7\n",
    "\n",
    "        #       Prepare upstream boundary (top segments of head basin reaches) data            \n",
    "        #---------------------------------------------------------------------------------\n",
    "            nts_ub_g= nts_ql_g \n",
    "            ubcd_g = fpm.fp_ubcd_map(frnw_g\n",
    "                                    , pynw\n",
    "                                    , nts_ub_g\n",
    "                                    , nrch_g\n",
    "                                    , ch_geo_data_tw\n",
    "                                    , qlat_tw\n",
    "                                    , qlat_g\n",
    "                                    )\n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-8\n",
    "\n",
    "        #       Prepare downstrea boundary (bottom segments of TW reaches) data            \n",
    "        #---------------------------------------------------------------------------------        \n",
    "            #import pdb; pdb.set_trace()\n",
    "            if tw in seg2usgsID:\n",
    "                ipos= seg2usgsID.index(tw)\n",
    "                usgsID2tw= usgsID[ipos]         \n",
    "                nts_db_g, dbcd_g=fpm.fp_dbcd_map(usgsID2tw\n",
    "                            , usgssDT\n",
    "                            , usgseDT\n",
    "                            , usgspCd\n",
    "                            )\n",
    "            else:\n",
    "                # no usgs data available at this TW.\n",
    "                nts_db_g=-1.0\n",
    "                \n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-8\n",
    "\n",
    "        #                 Prepare uniform flow lookup tables            \n",
    "        #---------------------------------------------------------------------------------          \n",
    "            #nhincr_m_g=20\n",
    "            #nhincr_f_g=20               \n",
    "            #timesdepth_g=10\n",
    "            nhincr_m_g= diffusive_parameters.get(\"normaldepth_lookuptable_main_increment_number\",None) \n",
    "            nhincr_f_g= diffusive_parameters.get(\"normaldepth_lookuptable_floodplain_increment_number\",None) \n",
    "            timesdepth_g= diffusive_parameters.get(\"normaldepth_lookuptable_depth_multiplier\",None)\n",
    "            ufqlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))                        \n",
    "            ufhlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))\n",
    "            ufqlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))\n",
    "            ufhlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))\n",
    "            #ufhlt_m_g, ufqlt_m_g, ufhlt_f_g, ufqlt_f_g= uniflow_lookuptable(mxncomp_g \n",
    "            #                                                            , nrch_g \n",
    "            #                                                            , bo_ar_g \n",
    "            #                                                            , traps_ar_g \n",
    "            #                                                            , tw_ar_g \n",
    "            #                                                            , twcc_ar_g \n",
    "            #                                                            , mann_ar_g \n",
    "            #                                                            , manncc_ar_g \n",
    "            #                                                            , so_ar_g \n",
    "            #                                                            , nhincr_m_g \n",
    "            #                                                            , nhincr_f_g               \n",
    "            #                                                            , frnw_col \n",
    "            #                                                            , dfrnw_g \n",
    "            #                                                            , timesdepth_g)\n",
    "\n",
    "        #---------------------------------------------------------------------------------\n",
    "        #                              Step 0-9\n",
    "\n",
    "        #                       Run diffusive model            \n",
    "        #---------------------------------------------------------------------------------  \n",
    "            ntss_ev_g= int((tfin_g - t0_g)*3600.0/saveinterval_ev_g)+1 \n",
    "            q_ev_g=np.zeros((ntss_ev_g, mxncomp_g, nrch_g))\n",
    "            elv_ev_g=np.zeros((ntss_ev_g, mxncomp_g, nrch_g))\n",
    "            \n",
    "            # build a dictionary of diffusive model inputs\n",
    "            diff_ins = {}\n",
    "            diff_ins[\"dtini_g\"] = dtini_g\n",
    "            diff_ins[\"t0_g\"] = t0_g\n",
    "            diff_ins[\"tfin_g\"] = tfin_g\n",
    "            diff_ins[\"saveinterval_g\"] = saveinterval_g\n",
    "            diff_ins[\"saveinterval_ev_g\"] = saveinterval_ev_g\n",
    "            diff_ins[\"dt_ql_g\"] = dt_ql_g\n",
    "            diff_ins[\"dt_ub_g\"] = dt_ub_g\n",
    "            diff_ins[\"dt_db_g\"] = dt_db_g\n",
    "            diff_ins[\"nts_ql_g\"] = nts_ql_g\n",
    "            diff_ins[\"nts_ub_g\"] = nts_ub_g\n",
    "            diff_ins[\"nts_db_g\"] = nts_db_g\n",
    "            diff_ins[\"mxncomp_g\"] = mxncomp_g\n",
    "            diff_ins[\"nrch_g\"] = nrch_g\n",
    "            diff_ins[\"z_ar_g\"] = z_ar_g\n",
    "            diff_ins[\"bo_ar_g\"] = bo_ar_g\n",
    "            diff_ins[\"traps_ar_g\"] = traps_ar_g\n",
    "            diff_ins[\"tw_ar_g\"] = tw_ar_g\n",
    "            diff_ins[\"twcc_ar_g\"] = twcc_ar_g\n",
    "            diff_ins[\"mann_ar_g\"] = mann_ar_g\n",
    "            diff_ins[\"manncc_ar_g\"] = manncc_ar_g\n",
    "            diff_ins[\"so_ar_g\"] = so_ar_g\n",
    "            diff_ins[\"dx_ar_g\"] = dx_ar_g\n",
    "            diff_ins[\"nhincr_m_g\"] = nhincr_m_g\n",
    "            diff_ins[\"nhincr_f_g\"] = nhincr_f_g\n",
    "            diff_ins[\"ufhlt_m_g\"] = ufhlt_m_g\n",
    "            diff_ins[\"ufqlt_m_g\"] = ufqlt_m_g\n",
    "            diff_ins[\"ufhlt_f_g\"] = ufhlt_f_g\n",
    "            diff_ins[\"ufqlt_f_g\"] = ufqlt_f_g\n",
    "            diff_ins[\"frnw_col\"] = frnw_col\n",
    "            diff_ins[\"frnw_g\"] = frnw_g\n",
    "            diff_ins[\"qlat_g\"] = qlat_g\n",
    "            diff_ins[\"ubcd_g\"] = ubcd_g\n",
    "            diff_ins[\"dbcd_g\"] = dbcd_g\n",
    "            diff_ins[\"cfl_g\"] = cfl_g\n",
    "            diff_ins[\"theta_g\"] = theta_g\n",
    "            diff_ins[\"tzeq_flag_g\"] = tzeq_flag_g\n",
    "            diff_ins[\"y_opt_g\"] = y_opt_g\n",
    "            diff_ins[\"so_llm_g\"] = so_llm_g\n",
    "            diff_ins[\"ntss_ev_g\"] = ntss_ev_g\n",
    "            \n",
    "            # save input data as yaml\n",
    "            import yaml\n",
    "            with open('diff_inputs.yml', 'w') as outfile:\n",
    "                yaml.dump(diff_ins, outfile, default_flow_style=False)\n",
    "            \n",
    "    print(\"input data preparation for diffusive routing complete\")\n",
    "    print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "def _input_handler():\n",
    "\n",
    "    #args = _handle_args()\n",
    "\n",
    "    #custom_input_file = args.custom_input_file\n",
    "    custom_input_file=\"../../test/input/yaml/CustomInput_florence_933020089_dt300.yaml\"\n",
    "    \n",
    "    supernetwork_parameters = {}\n",
    "    waterbody_parameters = {}\n",
    "    forcing_parameters = {}\n",
    "    restart_parameters = {}\n",
    "    output_parameters = {}\n",
    "    run_parameters = {}\n",
    "    parity_parameters = {}\n",
    "    diffusive_parameters={}\n",
    "\n",
    "    if custom_input_file:\n",
    "        (\n",
    "            supernetwork_parameters,\n",
    "            waterbody_parameters,\n",
    "            forcing_parameters,\n",
    "            restart_parameters,\n",
    "            output_parameters,\n",
    "            run_parameters,\n",
    "            parity_parameters,\n",
    "            diffusive_parameters,\n",
    "        ) = nhd_io.read_custom_input(custom_input_file)\n",
    "        run_parameters[\"debuglevel\"] *= -1\n",
    "        print(f\"in here: custom_input_file\")\n",
    "\n",
    "    else:\n",
    "        print(f\"deleted here\")\n",
    "    \n",
    "    return (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,\n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "        parity_parameters,\n",
    "        diffusive_parameters,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "     \n",
    "    (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,         \n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "        parity_parameters,\n",
    "        diffusive_parameters,\n",
    "    ) = _input_handler()\n",
    "   \n",
    "    dt = run_parameters.get(\"dt\", None)\n",
    "    nts = run_parameters.get(\"nts\", None)\n",
    "    verbose = run_parameters.get(\"verbose\", None)\n",
    "    showtiming = run_parameters.get(\"showtiming\", None)\n",
    "    debuglevel = run_parameters.get(\"debuglevel\", 0)\n",
    "    \n",
    "\n",
    "    geo_file_path = supernetwork_parameters.get(\"geo_file_path\", None)\n",
    "    print(f\"geo_file_path:{geo_file_path}\")    \n",
    "\n",
    "    if verbose:\n",
    "        print(\"creating supernetwork connections set\")\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 1: Build basic network connections graph\n",
    "    print(supernetwork_parameters)\n",
    "    connections, param_df = nnu.build_connections(supernetwork_parameters, dt)\n",
    "    #print(connections)\n",
    "     \n",
    "    if verbose:\n",
    "        print(\"supernetwork connections set complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    # STEP 2: Identify Independent Networks and Reaches by Network\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"organizing connections into reaches ...\")\n",
    "\n",
    "    independent_networks, reaches_bytw, rconn = nnu.organize_independent_networks(\n",
    "        connections\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"reach organization complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    # STEP 4: Handle Channel Initial States\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"setting channel initial states ...\")\n",
    "\n",
    "    q0 = nnu.build_channel_initial_state(restart_parameters, param_df.index)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"channel initial states complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 5: Read (or set) QLateral Inputs\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"creating qlateral array ...\")\n",
    "\n",
    "    qlats = nnu.build_qlateral_array(forcing_parameters, connections.keys(), nts)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"qlateral array complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    ################### Main Execution Loop across ordered networks\n",
    "    if showtiming:\n",
    "        main_start_time = time.time()\n",
    "    if verbose:\n",
    "        print(f\"executing routing computation ...\")\n",
    "        \n",
    "    \n",
    "    # Prepare input datasets for diffusive routing\n",
    "    diffusive_input_data_v02(connections\n",
    "                            , rconn\n",
    "                            , reaches_bytw\n",
    "                            , diffusive_parameters\n",
    "                            , param_df \n",
    "                            , qlats\n",
    "                            )\n",
    "\n",
    "    #if run_parameters.get(\"compute_method\", None) == \"standard cython compute network\":\n",
    "    #    compute_func = mc_reach.compute_network\n",
    "    #else:\n",
    "    #    compute_func = mc_reach.compute_network\n",
    "\n",
    "    #results = diffusive_routing_v02(\n",
    "    #    connections,\n",
    "    #    rconn,\n",
    "    #    reaches_bytw,\n",
    "    #    compute_func,\n",
    "    #    run_parameters.get(\"parallel_compute_method\", None),\n",
    "    #    run_parameters.get(\"subnetwork_target_size\", 1),\n",
    "        # The default here might be the whole network or some percentage...\n",
    "    #    run_parameters.get(\"cpu_pool\", None),\n",
    "    #    run_parameters.get(\"nts\", 1),\n",
    "    #    run_parameters.get(\"qts_subdivisions\", 1),\n",
    "    #    independent_networks,\n",
    "    #    param_df,\n",
    "    #    qlats,\n",
    "    #    q0,\n",
    "    #    run_parameters.get(\"assume_short_ts\", False),\n",
    "    #)\n",
    "\n",
    "    #csv_output_folder = output_parameters.get(\"csv_output_folder\", None)\n",
    "    #if (debuglevel <= -1) or csv_output_folder:\n",
    "    #    qvd_columns = pd.MultiIndex.from_product(\n",
    "    #        [range(nts), [\"q\", \"v\", \"d\"]]\n",
    "    #    ).to_flat_index()\n",
    "    #    flowveldepth = pd.concat(\n",
    "    #        [pd.DataFrame(d, index=i, columns=qvd_columns) for i, d in results],\n",
    "    #        copy=False,\n",
    "    #    )\n",
    "\n",
    "    #    if csv_output_folder:\n",
    "    #        flowveldepth = flowveldepth.sort_index()\n",
    "    #        output_path = pathlib.Path(csv_output_folder).resolve()\n",
    "    #        flowveldepth.to_csv(output_path.joinpath(f\"{args.supernetwork}.csv\"))\n",
    "\n",
    "    #    if debuglevel <= -1:\n",
    "    #        print(flowveldepth)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ordered reach computation complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
