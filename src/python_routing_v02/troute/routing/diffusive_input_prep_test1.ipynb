{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root = os.path.dirname(os.path.dirname(os.path.abspath('..')))\n",
    "sys.path.append(os.path.join(root, r'nwm_routing',r'src',r'nwm_routing'))\n",
    "sys.path.append(os.path.join(root, r'python_routing_v02'))\n",
    "#sys.path.append(os.path.join(root, r'python_routing_v02',r'troute'))\n",
    "#print(sys.path)\n",
    "#print(sys.path)\n",
    "#import main as m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_input_handler_v02() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59048c492872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mmain_v02\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-59048c492872>\u001b[0m in \u001b[0;36mmain_v02\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdiffusive_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcoastal_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     ) = _input_handler_v02()\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mrconn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _input_handler_v02() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "root = os.path.dirname(os.path.dirname(os.path.abspath('..')))\n",
    "sys.path.append(os.path.join(root, r'nwm_routing',r'src',r'nwm_routing'))\n",
    "sys.path.append(os.path.join(root, r'python_routing_v02'))\n",
    "sys.path.append(os.path.join(root, r'python_framework_v02'))\n",
    "\n",
    "## network and reach utilities\n",
    "import troute.nhd_network as nhd_network\n",
    "import troute.nhd_io as nhd_io\n",
    "import troute.nhd_network_utilities_v02 as nnu\n",
    "#import build_tests  # TODO: Determine whether and how to incorporate this into setup.py\n",
    "import troute.routing.diffusive_utils as diff_utils\n",
    "\n",
    "from input import _input_handler_v02, _input_handler_v03\n",
    "from preprocess import (\n",
    "    nwm_network_preprocess,\n",
    "    nwm_initial_warmstate_preprocess,\n",
    "    nwm_forcing_preprocess,\n",
    ")\n",
    "#from .output import nwm_output_generator\n",
    "#from troute.routing.compute import compute_nhd_routing_v02\n",
    "\n",
    "rconn=None\n",
    "connections=None\n",
    "\n",
    "def main_v02():\n",
    "    #args = _handle_args_v02(argv)\n",
    "    (\n",
    "        supernetwork_parameters,\n",
    "        waterbody_parameters,\n",
    "        forcing_parameters,\n",
    "        restart_parameters,\n",
    "        output_parameters,\n",
    "        run_parameters,\n",
    "        parity_parameters,\n",
    "        data_assimilation_parameters,\n",
    "        diffusive_parameters,\n",
    "        coastal_parameters,\n",
    "    ) = _input_handler_v02()\n",
    "    \n",
    "    global rconn\n",
    "    global connections\n",
    "\n",
    "    dt = run_parameters.get(\"dt\", None)\n",
    "    nts = run_parameters.get(\"nts\", None)\n",
    "    verbose = run_parameters.get(\"verbose\", None)\n",
    "    showtiming = run_parameters.get(\"showtiming\", None)\n",
    "    debuglevel = run_parameters.get(\"debuglevel\", 0)\n",
    "    break_network_at_waterbodies = run_parameters.get(\n",
    "        \"break_network_at_waterbodies\", False\n",
    "    )\n",
    "\n",
    "    if showtiming:\n",
    "        main_start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"creating supernetwork connections set\")\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 1: Build basic network connections graph\n",
    "    connections, param_df, wbodies, gages = nnu.build_connections(\n",
    "        supernetwork_parameters\n",
    "    )\n",
    "    if break_network_at_waterbodies:\n",
    "        connections = nhd_network.replace_waterbodies_connections(connections, wbodies)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"supernetwork connections set complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    ################################\n",
    "    ## STEP 3a: Read waterbody parameter file\n",
    "    # waterbodies_values = supernetwork_values[12]\n",
    "    # waterbodies_segments = supernetwork_values[13]\n",
    "    # connections_tailwaters = supernetwork_values[4]\n",
    "\n",
    "    waterbody_type_specified = False\n",
    "\n",
    "    if break_network_at_waterbodies:\n",
    "        # Read waterbody parameters\n",
    "        waterbodies_df = nhd_io.read_waterbody_df(\n",
    "            waterbody_parameters, {\"level_pool\": wbodies.values()}\n",
    "        )\n",
    "\n",
    "        # Remove duplicate lake_ids and rows\n",
    "        waterbodies_df = (\n",
    "            waterbodies_df.reset_index()\n",
    "            .drop_duplicates(subset=\"lake_id\")\n",
    "            .set_index(\"lake_id\")\n",
    "        )\n",
    "\n",
    "        #Declare empty dataframe\n",
    "        waterbody_types_df = pd.DataFrame()\n",
    "\n",
    "        #Check if hybrid-usgs, hybrid-usace, or rfc type reservoirs are set to true\n",
    "        wbtype=\"hybrid_and_rfc\"\n",
    "        wb_params_hybrid_and_rfc = waterbody_parameters.get(wbtype, defaultdict(list))  # TODO: Convert these to `get` statments\n",
    "\n",
    "        wbtype=\"level_pool\"\n",
    "        wb_params_level_pool = waterbody_parameters.get(wbtype, defaultdict(list))  # TODO: Convert these to `get` statments\n",
    "\n",
    "        waterbody_type_specified = False\n",
    "\n",
    "        if wb_params_hybrid_and_rfc[\"reservoir_persistence_usgs\"] \\\n",
    "        or wb_params_hybrid_and_rfc[\"reservoir_persistence_usace\"] \\\n",
    "        or wb_params_hybrid_and_rfc[\"reservoir_rfc_forecasts\"]:\n",
    "\n",
    "            waterbody_type_specified = True\n",
    "\n",
    "            waterbody_types_df = nhd_io.read_reservoir_parameter_file(wb_params_hybrid_and_rfc[\"reservoir_parameter_file\"], \\\n",
    "                wb_params_level_pool[\"level_pool_waterbody_id\"], wbodies.values(),) \n",
    "\n",
    "            # Remove duplicate lake_ids and rows\n",
    "            waterbody_types_df = (\n",
    "                waterbody_types_df.reset_index()\n",
    "                .drop_duplicates(subset=\"lake_id\")\n",
    "                .set_index(\"lake_id\")\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        #Declare empty dataframe\n",
    "        waterbody_types_df = pd.DataFrame()\n",
    "        waterbodies_df = pd.DataFrame()\n",
    "\n",
    "    # STEP 2: Identify Independent Networks and Reaches by Network\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"organizing connections into reaches ...\")\n",
    "\n",
    "    independent_networks, reaches_bytw, rconn = nnu.organize_independent_networks(\n",
    "        connections,\n",
    "        list(waterbodies_df.index.values)\n",
    "        if break_network_at_waterbodies\n",
    "        else None,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"reach organization complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    if break_network_at_waterbodies:\n",
    "        ## STEP 3c: Handle Waterbody Initial States\n",
    "        # TODO: move step 3c into function in nnu, like other functions wrapped in main()\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"setting waterbody initial states ...\")\n",
    "\n",
    "        if restart_parameters.get(\"wrf_hydro_waterbody_restart_file\", None):\n",
    "            waterbodies_initial_states_df = nhd_io.get_reservoir_restart_from_wrf_hydro(\n",
    "                restart_parameters[\"wrf_hydro_waterbody_restart_file\"],\n",
    "                restart_parameters[\"wrf_hydro_waterbody_ID_crosswalk_file\"],\n",
    "                restart_parameters[\"wrf_hydro_waterbody_ID_crosswalk_file_field_name\"],\n",
    "                restart_parameters[\"wrf_hydro_waterbody_crosswalk_filter_file\"],\n",
    "                restart_parameters[\n",
    "                    \"wrf_hydro_waterbody_crosswalk_filter_file_field_name\"\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            # TODO: Consider adding option to read cold state from route-link file\n",
    "            waterbodies_initial_ds_flow_const = 0.0\n",
    "            waterbodies_initial_depth_const = -1.0\n",
    "            # Set initial states from cold-state\n",
    "            waterbodies_initial_states_df = pd.DataFrame(\n",
    "                0, index=waterbodies_df.index, columns=[\"qd0\", \"h0\",], dtype=\"float32\"\n",
    "            )\n",
    "            # TODO: This assignment could probably by done in the above call\n",
    "            waterbodies_initial_states_df[\"qd0\"] = waterbodies_initial_ds_flow_const\n",
    "            waterbodies_initial_states_df[\"h0\"] = waterbodies_initial_depth_const\n",
    "            waterbodies_initial_states_df[\"index\"] = range(\n",
    "                len(waterbodies_initial_states_df)\n",
    "            )\n",
    "\n",
    "        waterbodies_df = pd.merge(\n",
    "            waterbodies_df, waterbodies_initial_states_df, on=\"lake_id\"\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"waterbody initial states complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "    # STEP 4: Handle Channel Initial States\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"setting channel initial states ...\")\n",
    "\n",
    "    q0 = nnu.build_channel_initial_state(restart_parameters, param_df.index)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"channel initial states complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "\n",
    "    # STEP 5: Read (or set) QLateral Inputs\n",
    "    if showtiming:\n",
    "        start_time = time.time()\n",
    "    if verbose:\n",
    "        print(\"creating qlateral array ...\")\n",
    "\n",
    "    forcing_parameters[\"qts_subdivisions\"] = run_parameters[\"qts_subdivisions\"]\n",
    "    forcing_parameters[\"nts\"] = run_parameters[\"nts\"]\n",
    "    qlats = nnu.build_qlateral_array(\n",
    "        forcing_parameters,\n",
    "        param_df.index,\n",
    "        nts,\n",
    "        run_parameters.get(\"qts_subdivisions\", 1),\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"qlateral array complete\")\n",
    "    if showtiming:\n",
    "        print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    # STEP 6\n",
    "    data_assimilation_csv = data_assimilation_parameters.get(\n",
    "        \"data_assimilation_csv\", None\n",
    "    )\n",
    "    data_assimilation_filter = data_assimilation_parameters.get(\n",
    "        \"data_assimilation_filter\", None\n",
    "    )\n",
    "    if data_assimilation_csv or data_assimilation_filter:\n",
    "        if showtiming:\n",
    "            start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"creating usgs time_slice data array ...\")\n",
    "\n",
    "        usgs_df, _ = nnu.build_data_assimilation(data_assimilation_parameters)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"usgs array complete\")\n",
    "        if showtiming:\n",
    "            print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    else:\n",
    "        usgs_df = pd.DataFrame()\n",
    "\n",
    "    last_obs_file = data_assimilation_parameters.get(\"wrf_hydro_last_obs_file\", None)\n",
    "    last_obs_df = pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_v02()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from troute.routing import diffusive_utils as diff_utils\n",
    "import troute.nhd_network_utilities_v02 as nnu\n",
    "import troute.nhd_network as nhd_network\n",
    "if 1==1:\n",
    "    # segment connections dictionary\n",
    "    connections = nhd_network.reverse_network(rconn)\n",
    "\n",
    "    # network tailwater\n",
    "    tw = list(nhd_network.headwaters(rconn))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # network reaches    \n",
    "    reach_list = []\n",
    "    for i in reaches_wTypes:\n",
    "        reach_list.append(i[0])\n",
    "\n",
    "    # generate diffusive inputs\n",
    "    diff_inputs = diff_utils.diffusive_input_data_v02(\n",
    "        tw,\n",
    "        connections,\n",
    "        rconn,\n",
    "        reach_list,\n",
    "        diffusive_parameters,\n",
    "        np.asarray(data_cols),\n",
    "        np.asarray(data_idx),\n",
    "        np.asarray(data_values),\n",
    "        np.asarray(qlat_values)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext cython\n",
    "#%%cython\n",
    "\n",
    "import cython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "cimport numpy as np\n",
    "\n",
    "from .fortran_wrappers cimport c_diffnw\n",
    "from .. import diffusive_utils as diff_utils\n",
    "import troute.nhd_network_utilities_v02 as nnu\n",
    "import troute.nhd_network as nhd_network\n",
    "\n",
    "# TO DO load some example inputs to test the module\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef void diffnw(double dtini_g,\n",
    "             double t0_g,\n",
    "             double tfin_g,\n",
    "             double saveinterval_g,\n",
    "             double saveinterval_ev_g,\n",
    "             double dt_ql_g,\n",
    "             double dt_ub_g,\n",
    "             double dt_db_g,\n",
    "             int nts_ql_g,\n",
    "             int nts_ub_g,\n",
    "             int nts_db_g,\n",
    "             int mxncomp_g,\n",
    "             int nrch_g,\n",
    "             double[::1,:] z_ar_g,\n",
    "             double[::1,:] bo_ar_g,\n",
    "             double[::1,:] traps_ar_g,\n",
    "             double[::1,:] tw_ar_g,\n",
    "             double[::1,:] twcc_ar_g,\n",
    "             double[::1,:] mann_ar_g,\n",
    "             double[::1,:] manncc_ar_g,\n",
    "             double[::1,:] so_ar_g,\n",
    "             double[::1,:] dx_ar_g,\n",
    "             int nhincr_m_g,\n",
    "             int nhincr_f_g,\n",
    "             double[::1,:,:] ufhlt_m_g,\n",
    "             double[::1,:,:] ufqlt_m_g,\n",
    "             double[::1,:,:] ufhlt_f_g,\n",
    "             double[::1,:,:] ufqlt_f_g,\n",
    "             int frnw_col,\n",
    "             double[::1,:] frnw_g,\n",
    "             double[::1,:,:] qlat_g,\n",
    "             double[::1,:] ubcd_g,\n",
    "             double[::1] dbcd_g,\n",
    "             double cfl_g,\n",
    "             double theta_g,\n",
    "             int tzeq_flag_g,\n",
    "             int y_opt_g,\n",
    "             double so_llm_g,\n",
    "             int ntss_ev_g,\n",
    "             double[:,:,:] out_q,\n",
    "             double[:,:,:] out_elv):\n",
    "\n",
    "    cdef:\n",
    "        double[::1,:,:] q_ev_g = np.empty([ntss_ev_g,mxncomp_g,nrch_g], dtype = np.double, order = 'F')\n",
    "        double[::1,:,:] elv_ev_g = np.empty([ntss_ev_g,mxncomp_g,nrch_g], dtype = np.double, order = 'F')\n",
    "\n",
    "    c_diffnw(\n",
    "            &dtini_g,\n",
    "            &t0_g,\n",
    "            &tfin_g,\n",
    "            &saveinterval_g,\n",
    "            &saveinterval_ev_g,\n",
    "            &dt_ql_g,\n",
    "            &dt_ub_g,\n",
    "            &dt_db_g,\n",
    "            &nts_ql_g,\n",
    "            &nts_ub_g,\n",
    "            &nts_db_g,\n",
    "            &mxncomp_g,\n",
    "            &nrch_g,\n",
    "            &z_ar_g[0,0],\n",
    "            &bo_ar_g[0,0],\n",
    "            &traps_ar_g[0,0],\n",
    "            &tw_ar_g[0,0],\n",
    "            &twcc_ar_g[0,0],\n",
    "            &mann_ar_g[0,0],\n",
    "            &manncc_ar_g[0,0],\n",
    "            &so_ar_g[0,0],\n",
    "            &dx_ar_g[0,0],\n",
    "            &nhincr_m_g,\n",
    "            &nhincr_f_g,\n",
    "            &ufhlt_m_g[0,0,0],\n",
    "            &ufqlt_m_g[0,0,0],\n",
    "            &ufhlt_f_g[0,0,0],\n",
    "            &ufqlt_f_g[0,0,0],\n",
    "            &frnw_col,\n",
    "            &frnw_g[0,0],\n",
    "            &qlat_g[0,0,0],\n",
    "            &ubcd_g[0,0],\n",
    "            &dbcd_g[0],\n",
    "            &cfl_g,\n",
    "            &theta_g,\n",
    "            &tzeq_flag_g,\n",
    "            &y_opt_g,\n",
    "            &so_llm_g,\n",
    "            &ntss_ev_g,\n",
    "            &q_ev_g[0,0,0],\n",
    "            &elv_ev_g[0,0,0])\n",
    "\n",
    "    # copy data from Fortran to Python memory view\n",
    "    out_q[:,:,:] = q_ev_g[::1,:,:]\n",
    "    out_elv[:,:,:] = elv_ev_g[::1,:,:]\n",
    "\n",
    "cpdef object compute_diffusive_tst(\n",
    "    int nsteps,\n",
    "    int qts_subdivisions,\n",
    "    list reaches_wTypes, # a list of tuples\n",
    "    dict rconn,\n",
    "    const long[:] data_idx,\n",
    "    object[:] data_cols,\n",
    "    const float[:,:] data_values,\n",
    "    const float[:,:] initial_conditions,\n",
    "    const float[:,:] qlat_values,\n",
    "    list lake_numbers_col,\n",
    "    const double[:,:] wbody_cols,\n",
    "    dict waterbody_parameters,\n",
    "    const int[:,:] reservoir_types,\n",
    "    bint reservoir_type_specified,\n",
    "    str model_start_time,\n",
    "    const float[:,:] usgs_values,\n",
    "    const int[:] usgs_positions_list,\n",
    "    const float[:,:] lastobs_values,\n",
    "    dict upstream_results={},\n",
    "    bint assume_short_ts=False,\n",
    "    bint return_courant=False,\n",
    "    dict diffusive_parameters=False\n",
    "    ):\n",
    "\n",
    "    # segment connections dictionary\n",
    "    connections = nhd_network.reverse_network(rconn)\n",
    "\n",
    "    # network tailwater\n",
    "    tw = list(nhd_network.headwaters(rconn))[0]\n",
    "\n",
    "    # network reaches\n",
    "    reach_list = []\n",
    "    for i in reaches_wTypes:\n",
    "        reach_list.append(i[0])\n",
    "\n",
    "    # generate diffusive inputs\n",
    "    diff_inputs = diff_utils.diffusive_input_data_v02(\n",
    "        tw,\n",
    "        connections,\n",
    "        rconn,\n",
    "        reach_list,\n",
    "        diffusive_parameters,\n",
    "        np.asarray(data_cols),\n",
    "        np.asarray(data_idx),\n",
    "        np.asarray(data_values),\n",
    "        np.asarray(qlat_values)\n",
    "        )\n",
    "\n",
    "    # unpack/declare diffusive input variables\n",
    "    cdef:\n",
    "        double dtini_g = diff_inputs[\"dtini_g\"]\n",
    "        double t0_g = diff_inputs[\"t0_g\"]\n",
    "        double tfin_g  = diff_inputs[\"tfin_g\"]\n",
    "        double saveinterval_g = diff_inputs[\"saveinterval_g\"]\n",
    "        double saveinterval_ev_g = diff_inputs[\"saveinterval_ev_g\"]\n",
    "        double dt_ql_g = diff_inputs[\"dt_ql_g\"]\n",
    "        double dt_ub_g = diff_inputs[\"dt_ub_g\"]\n",
    "        double dt_db_g = diff_inputs[\"dt_db_g\"]\n",
    "        int nts_ql_g = diff_inputs[\"nts_ql_g\"]\n",
    "        int nts_ub_g = diff_inputs[\"nts_ub_g\"]\n",
    "        int nts_db_g = diff_inputs[\"nts_db_g\"]\n",
    "        int mxncomp_g = diff_inputs[\"mxncomp_g\"]\n",
    "        int nrch_g = diff_inputs[\"nrch_g\"]\n",
    "        double[::1,:] z_ar_g = np.asfortranarray(diff_inputs[\"z_ar_g\"])\n",
    "        double[::1,:] bo_ar_g = np.asfortranarray(diff_inputs[\"bo_ar_g\"])\n",
    "        double[::1,:] traps_ar_g = np.asfortranarray(diff_inputs[\"traps_ar_g\"])\n",
    "        double[::1,:] tw_ar_g = np.asfortranarray(diff_inputs[\"tw_ar_g\"])\n",
    "        double[::1,:] twcc_ar_g = np.asfortranarray(diff_inputs[\"twcc_ar_g\"])\n",
    "        double[::1,:] mann_ar_g = np.asfortranarray(diff_inputs[\"mann_ar_g\"])\n",
    "        double[::1,:] manncc_ar_g = np.asfortranarray(diff_inputs[\"manncc_ar_g\"])\n",
    "        double[::1,:] so_ar_g = np.asfortranarray(diff_inputs[\"so_ar_g\"])\n",
    "        double[::1,:] dx_ar_g = np.asfortranarray(diff_inputs[\"dx_ar_g\"])\n",
    "        int nhincr_m_g = diff_inputs[\"nhincr_m_g\"]\n",
    "        int nhincr_f_g = diff_inputs[\"nhincr_f_g\"]\n",
    "        double[::1,:,:] ufhlt_m_g = np.asfortranarray(diff_inputs[\"ufhlt_m_g\"])\n",
    "        double[::1,:,:] ufqlt_m_g = np.asfortranarray(diff_inputs[\"ufqlt_m_g\"])\n",
    "        double[::1,:,:] ufhlt_f_g = np.asfortranarray(diff_inputs[\"ufhlt_f_g\"])\n",
    "        double[::1,:,:] ufqlt_f_g = np.asfortranarray(diff_inputs[\"ufqlt_f_g\"])\n",
    "        int frnw_col = diff_inputs[\"frnw_col\"]\n",
    "        double[::1,:] frnw_g = np.asfortranarray(diff_inputs[\"frnw_g\"], dtype = np.double)\n",
    "        double[::1,:,:] qlat_g = np.asfortranarray(diff_inputs[\"qlat_g\"])\n",
    "        double[::1,:] ubcd_g = np.asfortranarray(diff_inputs[\"ubcd_g\"])\n",
    "        double[::1] dbcd_g = np.asfortranarray(diff_inputs[\"dbcd_g\"])\n",
    "        double cfl_g = diff_inputs[\"cfl_g\"]\n",
    "        double theta_g = diff_inputs[\"theta_g\"]\n",
    "        int tzeq_flag_g = diff_inputs[\"tzeq_flag_g\"]\n",
    "        int y_opt_g = diff_inputs[\"y_opt_g\"]\n",
    "        double so_llm_g = diff_inputs[\"so_llm_g\"]\n",
    "        int ntss_ev_g = diff_inputs[\"ntss_ev_g\"]\n",
    "        double[:,:,:] out_q = np.empty([ntss_ev_g,mxncomp_g,nrch_g], dtype = np.double)\n",
    "        double[:,:,:] out_elv = np.empty([ntss_ev_g,mxncomp_g,nrch_g], dtype = np.double)\n",
    "\n",
    "    # call diffusive compute kernel\n",
    "    diffnw(dtini_g,\n",
    "     t0_g,\n",
    "     tfin_g,\n",
    "     saveinterval_g,\n",
    "     saveinterval_ev_g,\n",
    "     dt_ql_g,\n",
    "     dt_ub_g,\n",
    "     dt_db_g,\n",
    "     nts_ql_g,\n",
    "     nts_ub_g,\n",
    "     nts_db_g,\n",
    "     mxncomp_g,\n",
    "     nrch_g,\n",
    "     z_ar_g,\n",
    "     bo_ar_g,\n",
    "     traps_ar_g,\n",
    "     tw_ar_g,\n",
    "     twcc_ar_g,\n",
    "     mann_ar_g,\n",
    "     manncc_ar_g,\n",
    "     so_ar_g,\n",
    "     dx_ar_g,\n",
    "     nhincr_m_g,\n",
    "     nhincr_f_g,\n",
    "     ufhlt_m_g,\n",
    "     ufqlt_m_g,\n",
    "     ufhlt_f_g,\n",
    "     ufqlt_f_g,\n",
    "     frnw_col,\n",
    "     frnw_g,\n",
    "     qlat_g,\n",
    "     ubcd_g,\n",
    "     dbcd_g,\n",
    "     cfl_g,\n",
    "     theta_g,\n",
    "     tzeq_flag_g,\n",
    "     y_opt_g,\n",
    "     so_llm_g,\n",
    "     ntss_ev_g,\n",
    "     out_q,\n",
    "     out_elv)\n",
    "\n",
    "    # re-format outputs\n",
    "    index_array, flowvelelv = diff_utils.unpack_output(\n",
    "                                diff_inputs[\"pynw\"],\n",
    "                                diff_inputs[\"ordered_reaches\"],\n",
    "                                out_q,\n",
    "                                out_elv\n",
    "                                )\n",
    "\n",
    "    return index_array, flowvelelv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial, reduce\n",
    "import troute.nhd_network as nhd_network\n",
    "\n",
    "\n",
    "def adj_alt1(\n",
    "    mx_jorder, ordered_reaches, geo_cols, geo_index, geo_data, dbfksegID, z_all\n",
    "):\n",
    "    \"\"\"\n",
    "    Adjust reach altitude data so that altitude of last node in reach is equal to that of the head segment of\n",
    "    the neighboring downstream reach. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mx_jorder -- (int) maximum network reach order\n",
    "    geo_cols -- (ndarray of strs) column headers for geomorphic parameters data array (geo_data)\n",
    "    geo_index -- (ndarray of int64s) row indices for geomorphic parameters data array (geo_data)\n",
    "    geo_data --(ndarray of float32s) geomorphic parameters data arra\n",
    "    dbfksegID -- (int) segment ID of fake (ghost) node at network downstream boundary \n",
    "    z_all -- (dict) adjusted altitude dictionary with placeholder values to be replaced\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    z_all -- (dict) adjusted altitude dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    for x in range(mx_jorder, -1, -1):\n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            seg_list = reach[\"segments_list\"]\n",
    "            ncomp = reach[\"number_segments\"]\n",
    "            for seg in range(0, ncomp):\n",
    "                segID = seg_list[seg]\n",
    "                if seg == ncomp - 1 and seg_list.count(dbfksegID) == 0:\n",
    "                    # At junction, the altitude of fake segment of an upstream reach\n",
    "                    # is equal to that of the first segment of the downstream reach\n",
    "\n",
    "                    # head segment id of downstream reach from a junction\n",
    "                    dsrchID = reach[\"downstream_head_segment\"]\n",
    "\n",
    "                    idx_dsrchID = np.where(geo_index == dsrchID)\n",
    "                    idx_alt = np.where(geo_cols == \"alt\")\n",
    "                    z_all[segID][\"adj.alt\"][0] = geo_data[idx_dsrchID, idx_alt]\n",
    "\n",
    "                elif seg == ncomp - 1 and seg_list.count(dbfksegID) > 0:\n",
    "                    # Terminal downstream fakesegment\n",
    "                    ## AD HOC: need to be corrected later\n",
    "                    segID2 = seg_list[seg - 1]\n",
    "                    idx_segID2 = np.where(geo_index == segID2)\n",
    "                    idx_so = np.where(geo_cols == \"s0\")\n",
    "                    idx_dx = np.where(geo_cols == \"dx\")\n",
    "\n",
    "                    So = geo_data[idx_segID2, idx_so]\n",
    "                    dx = geo_data[idx_segID2, idx_dx]\n",
    "                    z_all[segID][\"adj.alt\"][0] = z_all[segID2][\"adj.alt\"][0] - So * dx\n",
    "                else:\n",
    "                    idx_segID = np.where(geo_index == segID)\n",
    "                    idx_alt = np.where(geo_cols == \"alt\")\n",
    "                    z_all[segID][\"adj.alt\"][0] = geo_data[idx_segID, idx_alt]\n",
    "\n",
    "    return z_all\n",
    "\n",
    "\n",
    "def fp_network_map(\n",
    "    mx_jorder, ordered_reaches, rchbottom_reaches, nrch_g, frnw_col, dbfksegID, pynw\n",
    "):\n",
    "    \"\"\"\n",
    "    Channel network mapping between Python and Fortran\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mx_jorder -- (int) maximum network reach order\n",
    "    ordered_reaches -- (dict) reaches and reach metadata by junction order\n",
    "    rchbottom_reaches -- (dict) reaches and reach metadata keyed by reach tail segment\n",
    "    nrch_g -- (int) number of reaches in the network\n",
    "    frnw_col -- (int) number of columns in the fortran network map\n",
    "    dbfskegID -- (str) segment ID of fake (ghost) node at network downstream boundary \n",
    "    pynw -- (dict) ordered reach head segments\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    frnw_g -- (nparray of int) Fortran-Python network mapping array\n",
    "    \"\"\"\n",
    "\n",
    "    #  Store headwater reach and upstream reaches above a junction\n",
    "    #  as well as downstream reach after a junction\n",
    "    #  into python-extension-fortran variables.\n",
    "    frnw_g = np.zeros((nrch_g, frnw_col), dtype=int)\n",
    "    frj = -1\n",
    "    for x in range(mx_jorder, -1, -1):\n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            seg_list = reach[\"segments_list\"]\n",
    "            ncomp = reach[\"number_segments\"]\n",
    "            frj = frj + 1\n",
    "            frnw_g[frj, 0] = ncomp\n",
    "\n",
    "            if not reach[\"upstream_bottom_segments\"]:\n",
    "                # headwater reach\n",
    "                frnw_g[frj, 2] = 0  # the number of upstream reaches\n",
    "            else:\n",
    "                # reaches before a junction\n",
    "                nusrch = len(reach[\"upstream_bottom_segments\"])\n",
    "                frnw_g[frj, 2] = nusrch  # the number of upstream reaches\n",
    "                usrch_bseg_list = list(reach[\"upstream_bottom_segments\"])\n",
    "                i = 0\n",
    "                for usrch in range(0, nusrch):\n",
    "                    usrch_bseg_id = usrch_bseg_list[\n",
    "                        usrch\n",
    "                    ]  # upstream reach's bottom segment\n",
    "                    usrch_hseg_id = rchbottom_reaches[usrch_bseg_id][\"segments_list\"][0]\n",
    "                    # find Fortran js corresponding to individual usrchid\n",
    "                    for j, sid in pynw.items():\n",
    "                        if sid == usrch_hseg_id:\n",
    "                            i = i + 1\n",
    "                            frnw_g[frj, 2 + i] = j\n",
    "\n",
    "            if seg_list.count(dbfksegID) > 0:\n",
    "                # a reach where downstream boundary condition is set.\n",
    "                frnw_g[\n",
    "                    frj, 1\n",
    "                ] = -100  # head_segment ID that is in terminal downstream reach.\n",
    "                # That is, -100 indicates the reach of the head segment is\n",
    "                # terminal downstream reach where ds.bcond. happens.\n",
    "            else:\n",
    "                # reach after a junction\n",
    "                dsrch_hseg_id = reach[\"downstream_head_segment\"]\n",
    "                # fortran j index equivalent to dsrchID.\n",
    "                frnw_g[frj, 1] = [\n",
    "                    j for j, sid in pynw.items() if sid == dsrch_hseg_id[0]\n",
    "                ][0]\n",
    "\n",
    "    # Adust frnw_g element values according to Fortran-Python index relationship, that is Python i = Fortran i+1\n",
    "    for frj in range(0, nrch_g):\n",
    "        frnw_g[frj, 1] = frnw_g[frj, 1] + 1  # downstream reach index for frj reach\n",
    "        if frnw_g[frj, 2] > 0:\n",
    "            nusrch = frnw_g[frj, 2]\n",
    "            for i in range(0, nusrch):\n",
    "                frnw_g[frj, 3 + i] = (\n",
    "                    frnw_g[frj, 3 + i] + 1\n",
    "                )  # upstream reach indicds for frj reach\n",
    "\n",
    "    return frnw_g\n",
    "\n",
    "\n",
    "def fp_chgeo_map(\n",
    "    mx_jorder, ordered_reaches, geo_cols, geo_index, geo_data, z_all, mxncomp_g, nrch_g\n",
    "):\n",
    "    \"\"\"\n",
    "    Channel geometry data mapping between Python and Fortran\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mx_jorder -- (int) maximum network reach order\n",
    "    ordered_reaches -- (dict) reaches and reach metadata by junction order\n",
    "    geo_cols -- (ndarray of strs) column headers for geomorphic parameters data array (geo_data)\n",
    "    geo_index -- (ndarray of int64s) row indices for geomorphic parameters data array (geo_data)\n",
    "    geo_data --(ndarray of float32s) geomorphic parameters data array\n",
    "    z_all -- (dict) adjusted altitude dictionary\n",
    "    mxncomp_g -- (int) maximum number of nodes in a reach\n",
    "    nrch_g -- (int) number of reaches in the network\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    z_ar_g -- (numpy of float64s) altitude (meters)\n",
    "    bo_ar_g -- (numpy of float64s) bottom width (meters)\n",
    "    traps_ar_g -- (numpy of float64s) sideslope (m/m)\n",
    "    tw_ar_g -- (numpy of float64s) top width (meters)\n",
    "    twcc_ar_g -- (numpy of float64s) top width of compound channel (meters)\n",
    "    mann_ar_g -- (numpy of float64s) manning's roughness (seconds/meters^1/3)\n",
    "    mancc_ar_g -- (numpy of float64s) manning's roughness compound channel (seconds/meters^(1/3))\n",
    "    so_ar_g -- (numpy of float64s) bottom slope (m/m)\n",
    "    dx_ar_g -- (numpy of float64s) segment length (meters)\n",
    "    \"\"\"\n",
    "\n",
    "    z_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    bo_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    traps_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    tw_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    twcc_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    mann_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    manncc_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    so_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    dx_ar_g = np.zeros((mxncomp_g, nrch_g))\n",
    "    frj = -1\n",
    "    for x in range(mx_jorder, -1, -1):\n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            seg_list = reach[\"segments_list\"]\n",
    "            ncomp = reach[\"number_segments\"]\n",
    "            frj = frj + 1\n",
    "            for seg in range(0, ncomp):\n",
    "                if seg == ncomp - 1:\n",
    "                    segID = seg_list[seg - 1]\n",
    "                else:\n",
    "                    segID = seg_list[seg]\n",
    "\n",
    "                idx_segID = np.where(geo_index == segID)\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"bw\")\n",
    "                bo_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"cs\")\n",
    "                traps_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"tw\")\n",
    "                tw_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"twcc\")\n",
    "                twcc_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"n\")\n",
    "                mann_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"ncc\")\n",
    "                manncc_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"s0\")\n",
    "                so_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                idx_par = np.where(geo_cols == \"dx\")\n",
    "                dx_ar_g[seg, frj] = geo_data[idx_segID, idx_par]\n",
    "\n",
    "                segID1 = seg_list[seg]\n",
    "                z_ar_g[seg, frj] = z_all[segID1][\"adj.alt\"][0]\n",
    "\n",
    "    return (\n",
    "        z_ar_g,\n",
    "        bo_ar_g,\n",
    "        traps_ar_g,\n",
    "        tw_ar_g,\n",
    "        twcc_ar_g,\n",
    "        mann_ar_g,\n",
    "        manncc_ar_g,\n",
    "        so_ar_g,\n",
    "        dx_ar_g,\n",
    "    )\n",
    "\n",
    "\n",
    "def fp_qlat_map(\n",
    "    mx_jorder,\n",
    "    ordered_reaches,\n",
    "    nts_ql_g,\n",
    "    geo_cols,\n",
    "    geo_index,\n",
    "    geo_data,\n",
    "    qlat_data,\n",
    "    qlat_g,\n",
    "):\n",
    "    \"\"\"\n",
    "    lateral inflow mapping between Python and Fortran\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mx_jorder -- (int) maximum network reach order\n",
    "    nts_ql_g -- (int) numer of qlateral timesteps\n",
    "    geo_cols -- (ndarray of strs) column headers for geomorphic parameters data array (geo_data)\n",
    "    geo_index -- (ndarray of int64) row indices for geomorphic parameters data array (geo_data)\n",
    "    geo_data -- (ndarray of float32) geomorphic parameters data array\n",
    "    qlat_data -- (ndarray of float32) qlateral data (m3/sec)\n",
    "    qlat_g -- (ndarray of float32) empty qlateral array to be filled\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    qlat_g -- (ndarray of float32) qlateral array (m3/sec/m)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    data in qlat_g are normalized by segment length with units of m2/sec = m3/sec/m\n",
    "    \"\"\"\n",
    "\n",
    "    frj = -1\n",
    "    for x in range(mx_jorder, -1, -1):\n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            seg_list = reach[\"segments_list\"]\n",
    "            ncomp = reach[\"number_segments\"]\n",
    "            frj = frj + 1\n",
    "            for seg in range(0, ncomp):\n",
    "                segID = seg_list[seg]\n",
    "                for tsi in range(0, nts_ql_g):\n",
    "                    if seg < ncomp - 1:\n",
    "\n",
    "                        idx_segID = np.where(geo_index == segID)\n",
    "                        idx_par = np.where(geo_cols == \"dx\")\n",
    "\n",
    "                        tlf = qlat_data[idx_segID, tsi]  # [m^3/sec]\n",
    "                        dx = geo_data[idx_segID, idx_par]  # [meter]\n",
    "                        qlat_g[tsi, seg, frj] = tlf / dx  # [m^2/sec]\n",
    "\n",
    "                    else:\n",
    "                        qlat_g[\n",
    "                            tsi, seg, frj\n",
    "                        ] = 0.0  # seg=ncomp is actually for bottom node in Fotran code.\n",
    "                        # And, lateral flow enters between adjacent nodes.\n",
    "\n",
    "    return qlat_g\n",
    "\n",
    "\n",
    "def fp_ubcd_map(frnw_g, pynw, nts_ub_g, nrch_g, geo_index, qlat_data, qlat_g):\n",
    "    \"\"\"\n",
    "    Upstream boundary condition mapping between Python and Fortran\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frnw_g -- (nparray of int) Fortran-Python network mapping array\n",
    "    pynw -- (dict) ordered reach head segments\n",
    "    nrch_g -- (int) number of reaches in the network\n",
    "    geo_index -- (ndarray of int64) row indices for geomorphic parameters data array (geo_data)\n",
    "    qlat_data -- (ndarray of float32) qlateral data (m3/sec)\n",
    "    qlat_g -- (ndarray of float32) qlateral array (m3/sec/m) \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ubcd_g -- (ndarray of float32) upstream boundary data (m3/sec)\n",
    "    \"\"\"\n",
    "\n",
    "    ubcd_g = np.zeros((nts_ub_g, nrch_g))\n",
    "    frj = -1\n",
    "    for frj in range(nrch_g):\n",
    "        if frnw_g[frj, 2] == 0:  # the number of upstream reaches is zero.\n",
    "            head_segment = pynw[frj]\n",
    "            for tsi in range(0, nts_ub_g):\n",
    "\n",
    "                idx_segID = np.where(geo_index == head_segment)\n",
    "\n",
    "                ubcd_g[tsi, frj] = qlat_data[idx_segID, tsi]  # [m^3/s]\n",
    "                qlat_g[tsi, 0, frj] = 0.0\n",
    "\n",
    "    return ubcd_g\n",
    "\n",
    "\n",
    "def fp_dbcd_map(usgsID2tw, usgssDT, usgseDT, usgspCd):\n",
    "    \"\"\"\n",
    "    Downststream boundary condition mapping between Python and Fortran using USGS stage observations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    usgsID2tw -- (str) usgs site ID\n",
    "    usgssDT -- (str) start data request date (yyyy-mm-dd) \n",
    "    usgseDT -- (str) end data request date (yyyy-mm-dd) \n",
    "    usgspCd --  (list of str) usgs parameter code \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nts_db_g -- (int) number of timesteps in downstream boundary data\n",
    "    dbcd_g -- (ndarray of float64) downstream boundary data (meters)\n",
    "    \"\"\"\n",
    "\n",
    "    # ** 1) downstream stage (here, lake elevation) boundary condition\n",
    "    # from nwis_client.iv import IVDataService\n",
    "    # install via: pip install hydrotools.nwis_client\n",
    "    try:\n",
    "        from hydrotools.nwis_client.iv import IVDataService\n",
    "    except ImportError as err:\n",
    "        print(err, end=\"... \")\n",
    "        print(\n",
    "            \"Please install hydrotools.nwis_client via: `pip install hydrotools.nwis_client`\"\n",
    "        )\n",
    "        raise  # ensures program exit by re-raising the error.\n",
    "\n",
    "    # from evaluation_tools.nwis_client.iv import IVDataService\n",
    "    # Retrieve streamflow and stage data from two sites\n",
    "    # Note: 1. Retrieved data all are based on UTC time zone (UTC is 4 hours ahead of Eastern Time during\n",
    "    #          daylight saving time and 5 hours ahead during standard time)\n",
    "    #       2. Retrieved data are always 1 hour ahead of stated startDT and 1 hour ahead of endDT,\n",
    "    #          where starDT or endDT equal to yyyy-mm-dd 00:00.\n",
    "    #       3. Also, retrieved data in 15 min so there are always four more data before startDT\n",
    "    #          and four less data before endDT.\n",
    "    #          For example, startDT='2018-08-01' and endDT='2018-09-01', then the retrieved data starts by\n",
    "    #          2018-07-31-23:00:00\n",
    "    #          2018-07-31-23:15:00\n",
    "    #          2018-07-31-23:30:00\n",
    "    #          2018-07-31-23:45:00\n",
    "    #          2018-08-01-00:00:00\n",
    "    #               .......\n",
    "    #          2018-08-31-22:00:00\n",
    "    #          2018-08-31-22:15:00\n",
    "    #          2018-08-31-22:30:00\n",
    "    #          2018-08-31-22:45:00\n",
    "    #          2018-08-31-23:00:00\n",
    "    #       4. '00060' for discharge [ft^3/s]\n",
    "    #          '00065' for stage [ft]\n",
    "    #          '62614' for Elevation, lake/res,NGVD29 [ft]\n",
    "    observations_data = IVDataService.get(\n",
    "        sites=usgsID2tw,  # sites='01646500,0208758850',\n",
    "        startDT=usgssDT,  #'2018-08-01',\n",
    "        endDT=usgseDT,  #'2020-09-01',\n",
    "        # parameterCd='62614'\n",
    "        parameterCd=usgspCd,\n",
    "    )\n",
    "    nts_db_g = len(observations_data) - 4\n",
    "    # ** 4 is added to make data used here has its date time as from startDT 00:00 to (endDT-1day) 23:00, UTC\n",
    "    # ** usgs data at this site uses NGVD1929 feet datum while 'alt' of RouteLink uses NAD88 meter datum.\n",
    "    #  -> Has to convert accordingly !!!!\n",
    "    #\n",
    "    # source: https://pubs.usgs.gov/sir/2010/5040/section.html\n",
    "    # Over most USGS study area it is used that NGVD = NAVD88 - 3.6 feet\n",
    "    dbcd_g = np.zeros(nts_db_g)\n",
    "    for tsi in range(0, nts_db_g):\n",
    "        i = tsi + 4\n",
    "        dmy = observations_data.iloc[i, 4]\n",
    "        dbcd_g[tsi] = 0.3048 * (\n",
    "            dmy + 3.6\n",
    "        )  # accuracy with +-0.5feet for 95 percent of USGS study area.\n",
    "        # 0.3048 to covert ft to meter. [meter]\n",
    "    return nts_db_g, dbcd_g\n",
    "\n",
    "\n",
    "def diffusive_input_data_v02(\n",
    "    tw,\n",
    "    connections,\n",
    "    rconn,\n",
    "    reach_list,\n",
    "    diffusive_parameters,\n",
    "    geo_cols,\n",
    "    geo_index,\n",
    "    geo_data,\n",
    "    qlat_data,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build input data objects for diffusive wave model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tw -- (int) Tailwater segment ID\n",
    "    connections -- (dict) donwstream connections for each segment in the network\n",
    "    rconn -- (dict) upstream connections for each segment in the network\n",
    "    reach_list -- (list of lists) lists of segments comprising different reaches in the network\n",
    "    diffusive_parametters -- (dict) Diffusive wave model parameters\n",
    "    geo_cols -- (ndarray of strs) column headers for geomorphic parameters data array (geo_data)\n",
    "    geo_index -- (ndarray of int64s) row indices for geomorphic parameters data array (geo_data)\n",
    "    geo_data --(ndarray of float32s) geomorphic parameters data array\n",
    "    qlat_data -- (ndarray of float32) qlateral data (m3/sec)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    diff_ins -- (dict) formatted inputs for diffusive wave model\n",
    "    \"\"\"\n",
    "\n",
    "    # diffusive time steps info.\n",
    "    dt_ql_g = diffusive_parameters.get(\"dt_qlat\", None)  # time step of lateral flow\n",
    "    dt_ub_g = diffusive_parameters.get(\n",
    "        \"dt_upstream_boundary\", None\n",
    "    )  # time step of us.boundary data\n",
    "    dt_db_g = diffusive_parameters.get(\n",
    "        \"dt_downstream_boundary\", None\n",
    "    )  # time step of ds.boundary data\n",
    "    saveinterval_g = diffusive_parameters.get(\n",
    "        \"dt_output\", None\n",
    "    )  # time step for outputting routed results\n",
    "    saveinterval_ev_g = diffusive_parameters.get(\n",
    "        \"dt_output\", None\n",
    "    )  # time step for evaluating routed results\n",
    "    dtini_g = diffusive_parameters.get(\n",
    "        \"dt_diffusive\", None\n",
    "    )  # initial simulation time step\n",
    "    t0_g = 0.0  # simulation start hr **set to zero for Fortran computation\n",
    "    tfin_g = diffusive_parameters.get(\"simulation_end_hr\", None)  # simulation end time\n",
    "\n",
    "    # USGS data related info.\n",
    "    usgsID = diffusive_parameters.get(\"usgsID\", None)\n",
    "    seg2usgsID = diffusive_parameters.get(\"link2usgsID\", None)\n",
    "    usgssDT = diffusive_parameters.get(\"usgs_start_date\", None)\n",
    "    usgseDT = diffusive_parameters.get(\"usgs_end_date\", None)\n",
    "    usgspCd = diffusive_parameters.get(\"usgs_parameterCd\", None)\n",
    "\n",
    "    # diffusive parameters\n",
    "    cfl_g = diffusive_parameters.get(\"courant_number_upper_limit\", None)\n",
    "    theta_g = diffusive_parameters.get(\"theta_parameter\", None)\n",
    "    tzeq_flag_g = diffusive_parameters.get(\"chgeo_computation_flag\", None)\n",
    "    y_opt_g = diffusive_parameters.get(\"water_elevation_computation_flag\", None)\n",
    "    so_llm_g = diffusive_parameters.get(\"bed_slope_lower_limit\", None)\n",
    "\n",
    "    # number of reaches in network\n",
    "    nrch_g = len(reach_list)\n",
    "\n",
    "    # maximum number of nodes in a reach\n",
    "    mxncomp_g = 0\n",
    "    for r in reach_list:\n",
    "        nnodes = len(r) + 1\n",
    "        if nnodes > mxncomp_g:\n",
    "            mxncomp_g = nnodes\n",
    "\n",
    "    # Order reaches by junction depth\n",
    "    path_func = partial(nhd_network.split_at_junction, rconn)\n",
    "    tr = nhd_network.dfs_decomposition_depth_tuple(rconn, path_func)\n",
    "    jorder_reaches = sorted(tr, key=lambda x: x[0])\n",
    "    mx_jorder = max(jorder_reaches)[0]  # maximum junction order of subnetwork of TW\n",
    "\n",
    "    ordered_reaches = {}\n",
    "    rchhead_reaches = {}\n",
    "    rchbottom_reaches = {}\n",
    "    z_all = {}\n",
    "    for o, rch in jorder_reaches:\n",
    "\n",
    "        # add one more segment(fake) to the end of a list of segments to account for node configuration.\n",
    "        fksegID = int(str(rch[-1]) + str(2))\n",
    "        rch.append(fksegID)\n",
    "\n",
    "        # additional segment(fake) to upstream bottom segments\n",
    "        fk_usbseg = [int(str(x) + str(2)) for x in rconn[rch[0]]]\n",
    "\n",
    "        if o not in ordered_reaches:\n",
    "            ordered_reaches.update({o: []})\n",
    "        ordered_reaches[o].append(\n",
    "            [\n",
    "                rch[0],\n",
    "                {\n",
    "                    \"number_segments\": len(rch),\n",
    "                    \"segments_list\": rch,\n",
    "                    \"upstream_bottom_segments\": fk_usbseg,\n",
    "                    \"downstream_head_segment\": connections[rch[-2]],\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if rch[0] not in rchhead_reaches:\n",
    "            # a list of segments for a given head segment\n",
    "            rchhead_reaches.update(\n",
    "                {rch[0]: {\"number_segments\": len(rch), \"segments_list\": rch}}\n",
    "            )\n",
    "            # a list of segments for a given bottom segment\n",
    "            rchbottom_reaches.update(\n",
    "                {rch[-1]: {\"number_segments\": len(rch), \"segments_list\": rch}}\n",
    "            )\n",
    "        # for channel altitude adjustment\n",
    "        z_all.update({seg: {\"adj.alt\": np.zeros(1)} for seg in rch})\n",
    "\n",
    "        # cahnnel geometry data\n",
    "        a = np.where(geo_cols == \"cs\")\n",
    "        geo_data[:, a] = 1.0 / geo_data[:, a]\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-3\n",
    "    #    Adjust altitude so that altitude of the last sement of a reach is equal to that\n",
    "    #    of the first segment of its downstream reach right after their common junction.\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    dbfksegID = int(str(tw) + str(2))\n",
    "\n",
    "    adj_alt1(\n",
    "        mx_jorder, ordered_reaches, geo_cols, geo_index, geo_data, dbfksegID, z_all\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-4\n",
    "    #     Make Fortran-Python channel network mapping variables.\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    # build a list of head segments in descending reach order [headwater -> tailwater]\n",
    "    pynw = {}\n",
    "    frj = -1\n",
    "    for x in range(mx_jorder, -1, -1):\n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            frj = frj + 1\n",
    "            pynw[frj] = head_segment\n",
    "\n",
    "    frnw_col = diffusive_parameters.get(\"fortran_nework_map_col_number\", None)\n",
    "    frnw_g = fp_network_map(\n",
    "        mx_jorder, ordered_reaches, rchbottom_reaches, nrch_g, frnw_col, dbfksegID, pynw\n",
    "    )\n",
    "\n",
    "    # covert data type from integer to float for frnw\n",
    "    dfrnw_g = np.zeros((nrch_g, frnw_col), dtype=float)\n",
    "    for j in range(0, nrch_g):\n",
    "        for col in range(0, frnw_col):\n",
    "            dfrnw_g[j, col] = float(frnw_g[j, col])\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-5\n",
    "    #                  Prepare channel geometry data\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    (\n",
    "        z_ar_g,\n",
    "        bo_ar_g,\n",
    "        traps_ar_g,\n",
    "        tw_ar_g,\n",
    "        twcc_ar_g,\n",
    "        mann_ar_g,\n",
    "        manncc_ar_g,\n",
    "        so_ar_g,\n",
    "        dx_ar_g,\n",
    "    ) = fp_chgeo_map(\n",
    "        mx_jorder,\n",
    "        ordered_reaches,\n",
    "        geo_cols,\n",
    "        geo_index,\n",
    "        geo_data,\n",
    "        z_all,\n",
    "        mxncomp_g,\n",
    "        nrch_g,\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-6\n",
    "\n",
    "    #                  Prepare lateral inflow data\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    nts_ql_g = (\n",
    "        int((tfin_g - t0_g) * 3600.0 / dt_ql_g) + 1\n",
    "    )  # the number of the entire time steps of lateral flow data\n",
    "\n",
    "    qlat_g = np.zeros((nts_ql_g, mxncomp_g, nrch_g))\n",
    "\n",
    "    fp_qlat_map(\n",
    "        mx_jorder,\n",
    "        ordered_reaches,\n",
    "        nts_ql_g,\n",
    "        geo_cols,\n",
    "        geo_index,\n",
    "        geo_data,\n",
    "        qlat_data,\n",
    "        qlat_g,\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-7\n",
    "\n",
    "    #       Prepare upstream boundary (top segments of head basin reaches) data\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    nts_ub_g = nts_ql_g\n",
    "    ubcd_g = fp_ubcd_map(frnw_g, pynw, nts_ub_g, nrch_g, geo_index, qlat_data, qlat_g)\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-8\n",
    "\n",
    "    #       Prepare downstrea boundary (bottom segments of TW reaches) data\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    if tw in seg2usgsID:\n",
    "        ipos = seg2usgsID.index(tw)\n",
    "        usgsID2tw = usgsID[ipos]\n",
    "        nts_db_g, dbcd_g = fp_dbcd_map(usgsID2tw, usgssDT, usgseDT, usgspCd)\n",
    "    else:\n",
    "        # no usgs data available at this TW.\n",
    "        nts_db_g = -1.0\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-8\n",
    "\n",
    "    #                 Prepare uniform flow lookup tables\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    nhincr_m_g = diffusive_parameters.get(\n",
    "        \"normaldepth_lookuptable_main_increment_number\", None\n",
    "    )\n",
    "    nhincr_f_g = diffusive_parameters.get(\n",
    "        \"normaldepth_lookuptable_floodplain_increment_number\", None\n",
    "    )\n",
    "    timesdepth_g = diffusive_parameters.get(\n",
    "        \"normaldepth_lookuptable_depth_multiplier\", None\n",
    "    )\n",
    "    ufqlt_m_g = np.zeros((mxncomp_g, nrch_g, nhincr_m_g))\n",
    "    ufhlt_m_g = np.zeros((mxncomp_g, nrch_g, nhincr_m_g))\n",
    "    ufqlt_f_g = np.zeros((mxncomp_g, nrch_g, nhincr_f_g))\n",
    "    ufhlt_f_g = np.zeros((mxncomp_g, nrch_g, nhincr_f_g))\n",
    "\n",
    "    # TODO: Call uniform flow lookup table creation kernel\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    #                              Step 0-9\n",
    "\n",
    "    #                       Build input dictionary\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    ntss_ev_g = int((tfin_g - t0_g) * 3600.0 / saveinterval_ev_g) + 1\n",
    "\n",
    "    # build a dictionary of diffusive model inputs and helper variables\n",
    "    diff_ins = {}\n",
    "\n",
    "    # model input parameters\n",
    "    diff_ins[\"dtini_g\"] = dtini_g\n",
    "    diff_ins[\"t0_g\"] = t0_g\n",
    "    diff_ins[\"tfin_g\"] = tfin_g\n",
    "    diff_ins[\"saveinterval_g\"] = saveinterval_g\n",
    "    diff_ins[\"saveinterval_ev_g\"] = saveinterval_ev_g\n",
    "    diff_ins[\"dt_ql_g\"] = dt_ql_g\n",
    "    diff_ins[\"dt_ub_g\"] = dt_ub_g\n",
    "    diff_ins[\"dt_db_g\"] = dt_db_g\n",
    "    diff_ins[\"nts_ql_g\"] = nts_ql_g\n",
    "    diff_ins[\"nts_ub_g\"] = nts_ub_g\n",
    "    diff_ins[\"nts_db_g\"] = nts_db_g\n",
    "    diff_ins[\"mxncomp_g\"] = mxncomp_g\n",
    "    diff_ins[\"nrch_g\"] = nrch_g\n",
    "    diff_ins[\"z_ar_g\"] = z_ar_g\n",
    "    diff_ins[\"bo_ar_g\"] = bo_ar_g\n",
    "    diff_ins[\"traps_ar_g\"] = traps_ar_g\n",
    "    diff_ins[\"tw_ar_g\"] = tw_ar_g\n",
    "    diff_ins[\"twcc_ar_g\"] = twcc_ar_g\n",
    "    diff_ins[\"mann_ar_g\"] = mann_ar_g\n",
    "    diff_ins[\"manncc_ar_g\"] = manncc_ar_g\n",
    "    diff_ins[\"so_ar_g\"] = so_ar_g\n",
    "    diff_ins[\"dx_ar_g\"] = dx_ar_g\n",
    "    diff_ins[\"nhincr_m_g\"] = nhincr_m_g\n",
    "    diff_ins[\"nhincr_f_g\"] = nhincr_f_g\n",
    "    diff_ins[\"ufhlt_m_g\"] = ufhlt_m_g\n",
    "    diff_ins[\"ufqlt_m_g\"] = ufqlt_m_g\n",
    "    diff_ins[\"ufhlt_f_g\"] = ufhlt_f_g\n",
    "    diff_ins[\"ufqlt_f_g\"] = ufqlt_f_g\n",
    "    diff_ins[\"frnw_col\"] = frnw_col\n",
    "    diff_ins[\"frnw_g\"] = frnw_g\n",
    "    diff_ins[\"qlat_g\"] = qlat_g\n",
    "    diff_ins[\"ubcd_g\"] = ubcd_g\n",
    "    diff_ins[\"dbcd_g\"] = dbcd_g\n",
    "    diff_ins[\"cfl_g\"] = cfl_g\n",
    "    diff_ins[\"theta_g\"] = theta_g\n",
    "    diff_ins[\"tzeq_flag_g\"] = tzeq_flag_g\n",
    "    diff_ins[\"y_opt_g\"] = y_opt_g\n",
    "    diff_ins[\"so_llm_g\"] = so_llm_g\n",
    "    diff_ins[\"ntss_ev_g\"] = ntss_ev_g\n",
    "\n",
    "    # python-fortran crosswalk data\n",
    "    diff_ins[\"pynw\"] = pynw\n",
    "    diff_ins[\"ordered_reaches\"] = ordered_reaches\n",
    "\n",
    "    return diff_ins\n",
    "\n",
    "\n",
    "def unpack_output(pynw, ordered_reaches, out_q, out_elv):\n",
    "    \"\"\"\n",
    "    Unpack diffusive wave output arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pynw -- (dict) ordered reach head segments\n",
    "    ordered_reaches -- \n",
    "    out_q -- (diffusive._memoryviewslice of float64) diffusive wave model flow output (m3/sec)\n",
    "    out_elv -- (diffusive._memoryviewslice of float64) diffusive wave model water surface elevation output (meters)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.asarray(rch_list, dtype=np.intp) - segment indices \n",
    "    np.asarray(dat_all, dtype = 'float32') - flow, velocity, elevation array\n",
    "    \"\"\"\n",
    "\n",
    "    reach_heads = list(pynw.values())\n",
    "    nts = len(out_q[:, 0, 0])\n",
    "\n",
    "    i = 1\n",
    "    rch_list = []\n",
    "    for o in ordered_reaches.keys():\n",
    "        for rch in ordered_reaches[o]:\n",
    "\n",
    "            rch_segs = rch[1][\"segments_list\"]\n",
    "            rch_list.extend(rch_segs)\n",
    "\n",
    "            j = reach_heads.index(rch[0])\n",
    "\n",
    "            if i == 1:\n",
    "                dat_all = np.empty((len(rch_segs), nts * 3))\n",
    "                dat_all[:] = np.nan\n",
    "                # flow result\n",
    "                dat_all[:, ::3] = np.transpose(np.array(out_q[:, 0 : len(rch_segs), j]))\n",
    "                # elevation result\n",
    "                dat_all[:, 2::3] = np.transpose(\n",
    "                    np.array(out_elv[:, 0 : len(rch_segs), j])\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                dat_all_c = np.empty((len(rch_segs), nts * 3))\n",
    "                dat_all_c[:] = np.nan\n",
    "                # flow result\n",
    "                dat_all_c[:, ::3] = np.transpose(\n",
    "                    np.array(out_q[:, 0 : len(rch_segs), j])\n",
    "                )\n",
    "                # elevation result\n",
    "                dat_all_c[:, 2::3] = np.transpose(\n",
    "                    np.array(out_elv[:, 0 : len(rch_segs), j])\n",
    "                )\n",
    "                # concatenate\n",
    "                dat_all = np.concatenate((dat_all, dat_all_c))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    return np.asarray(rch_list, dtype=np.intp), np.asarray(dat_all, dtype=\"float32\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
